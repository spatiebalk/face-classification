{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different classifiers on data of ADNP patients and ID controls\n",
    "Overview of this notebook:\n",
    "\n",
    "First the deepface representations of the cropped images are read in from an Excel file. The data is then plotted by using either t-sne or PCA for dimension reduction. It is clear that there aren't two clear clusters.\n",
    "\n",
    "In the rest of the notebook the following classifiers are tested: k-NN, SVM, Random Forest, Gradient Boosting, AdaBoost, Gaussian Naive Bayes. In the end also an ensemble of all these methods or some of them is tried. None outperforming the Gradient Boosting classifier. \n",
    "\n",
    "To normalize the data either Normalizer (unit form) or StandardScaler (z = (x - mean)/std) is used, without any specific difference in performance yet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rep(syn_csv, ID_csv, data_dir):\n",
    "    \n",
    "    # open directories\n",
    "    syn_dir = data_dir+\"\\\\{}-patients\".format(syn_name)\n",
    "    ID_dir = data_dir+ \"\\\\{}-selected-ID-controls\".format(syn_name)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_syn = [f for f in listdir(syn_dir) if (isfile(join(syn_dir, f)) )]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f)))]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, csv_file in enumerate([ID_csv, syn_csv]):\n",
    "        with open (csv_file, newline='') as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if row[0] in files_syn or row[0] in files_ID:\n",
    "                    rep = list(map(float, row[1:]))\n",
    "                    data.append(rep)\n",
    "                    labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nog verbeteren\n",
    "def read_rep2(kdv_csv, ID_csv, low_age, high_age, data_dir):\n",
    "    \n",
    "    # open directories\n",
    "    kdv_dir = data_dir+\"\\\\kdv-patients-age-group-\"+str(low_age) + \"-\" + str(high_age)\n",
    "    ID_dir = data_dir+ \"\\\\kdv-selected-ID-controls-age-group-\"+str(low_age) + \"-\" + str(high_age)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_kdv = [f for f in listdir(kdv_dir) if (isfile(join(kdv_dir, f)) & (\"crop_sized.jpg\" in f))]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f)) & (\"crop_sized.JPG\" in f))]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, csv_file in enumerate([ID_csv, kdv_csv]):\n",
    "        with open (csv_file, newline='') as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if row[0] in files_kdv or row[0] in files_ID:\n",
    "                    rep = list(map(float, row[1:]))\n",
    "                    data.append(row)\n",
    "                    labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rep_fr(kdv_csv, ID_csv, low_age, high_age, data_dir):\n",
    "    \n",
    "    # open directories\n",
    "    kdv_dir = data_dir+\"\\\\kdv-patients-age-group-\"+str(low_age) + \"-\" + str(high_age)\n",
    "    ID_dir = data_dir+ \"\\\\kdv-selected-ID-controls-age-group-\"+str(low_age) + \"-\" + str(high_age)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_kdv = [f for f in listdir(kdv_dir) if (isfile(join(kdv_dir, f)) & (\"crop\" not in f))]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f)) & (\"crop\" not in f))]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, csv_file in enumerate([ID_csv, kdv_csv]):\n",
    "        with open (csv_file, newline='') as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if row[0] in files_kdv or row[0] in files_ID:\n",
    "                    rep = list(map(float, row[1:]))\n",
    "                    data.append(rep)\n",
    "                    labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_tsne(data, labels, lowest_age = -1, highest_age = -1):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot([1,2])\n",
    "\n",
    "    # visualize data in tnse (men/women)\n",
    "    X_embedded_tsne = TSNE(n_components=2, init='pca').fit_transform(data)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    unique = list(set(labels))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [X_embedded_tsne[j, 0] for j  in range(len(X_embedded_tsne[:,0])) if labels[j] == u]\n",
    "        yi = [X_embedded_tsne[j, 1] for j  in range(len(X_embedded_tsne[:,1])) if labels[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors[i]], label=str(u))\n",
    "    plt.legend()\n",
    "    plt.title(\"t-sne for age range {}-{}\".format(lowest_age, highest_age))\n",
    "\n",
    "    # visualize data in pca (men/women)\n",
    "    X_embedded_pca = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    unique = list(set(labels))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [X_embedded_pca[j, 0] for j  in range(len(X_embedded_pca[:,0])) if labels[j] == u]\n",
    "        yi = [X_embedded_pca[j, 1] for j  in range(len(X_embedded_pca[:,1])) if labels[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors[i]], label=str(u))\n",
    "    plt.legend()\n",
    "    plt.title(\"pca for age range{}-{}\".format(lowest_age, highest_age))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred): \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    plt.figure(1, figsize=(12,6))\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='LOOCV ROC (AUC = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Chance level', alpha=.8)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def calculate_metrics_loo(model, data, labels):\n",
    "    all_y, all_probs, all_preds = [], [], [] \n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    for train, test in loo.split(data):\n",
    "        all_y.append(labels[test])\n",
    "        model = model.fit(data[train], labels[train])\n",
    "        all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "        all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "        \n",
    "    aroc = roc_auc_score(all_y, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn) \n",
    "    \n",
    "    return aroc, spec, sens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, i):\n",
    "\n",
    "    if i == 0:\n",
    "        return data\n",
    "    \n",
    "    if i == 1:\n",
    "        return Normalizer().fit_transform(data)\n",
    "        \n",
    "    if i == 2:\n",
    "        return StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(data, labels):\n",
    "    k_values = [3,5,7,9,11,13,15]\n",
    "    best_aroc = 0\n",
    "    best_k = 0\n",
    "\n",
    "    for k in tqdm(k_values):\n",
    "        for i in [0, 1, 2]:\n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_k = k\n",
    "                \n",
    "    return best_k, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(data, labels):\n",
    "    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    best_aroc = 0\n",
    "    best_kernel = None\n",
    "\n",
    "    for k in tqdm(kernels):\n",
    "        for i in [0, 1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = SVC(kernel=k, probability=True)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_kernel = k\n",
    "                \n",
    "    return best_kernel, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20] #, 40, 60, 80]\n",
    "    best_estimator_rf = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for i in [0, 1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = RandomForestClassifier(n_estimators=est)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_estimator_rf = est\n",
    "    \n",
    "    return best_estimator_rf, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20] #, 40, 60, 80]\n",
    "    best_estimator_gr = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for i in [0, 1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = GradientBoostingClassifier(n_estimators=est)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_estimator_gr = est\n",
    "                \n",
    "            if best_aroc > 0.8:\n",
    "                print(\"tn {}, fp {}, fn {}, tp {}\".format(tn, fp, fn, tp))\n",
    "                print(\"aroc: {} , spec: {}, sens: {}\".format(best_aroc, spec, sens))\n",
    "                print(\"trees: {}, norm: {}\".format(best_estimator_gr, best_norm))\n",
    "                conf_matrix = [[tp, fp],\n",
    "                             [fn, tn]]\n",
    "                df_cm = pd.DataFrame(conf_matrix, index = [\"Syn_pred\", \"Control_pred\"], columns = [\"Syn\", \"Control\"])\n",
    "                plt.figure(figsize = (6, 6))\n",
    "                sns_heat = sns.heatmap(df_cm, annot=True)\n",
    "                plt.show()                \n",
    "                \n",
    "                                \n",
    "    return best_estimator_gr, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20 ] #, 40, 60, 80]\n",
    "    best_estimator_ada = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for i in [0,1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = AdaBoostClassifier(n_estimators=est)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_estimator_ada = est\n",
    "                \n",
    "    return best_estimator_ada, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(data_dir, data_combination, nr_feats): \n",
    "\n",
    "    method = \"deepface\"\n",
    "    syn_csv = data_dir+\"\\\\representations\\{}-patients-{}.csv\".format(syn_name, method)\n",
    "    ID_csv  = data_dir+\"\\\\representations\\ID-controls-{}.csv\".format(method)\n",
    "    data_df, labels_df = read_rep(syn_csv, ID_csv, data_dir)\n",
    "    \n",
    "    method = \"dlib\"\n",
    "    syn_csv = data_dir+\"\\\\representations\\{}-patients-{}.csv\".format(syn_name, method)\n",
    "    ID_csv  = data_dir+\"\\\\representations\\ID-controls-{}.csv\".format(method)\n",
    "    data_dlib, labels_dlib = read_rep(syn_csv, ID_csv, data_dir)\n",
    "\n",
    "    \n",
    "    if data_combination == 0: # or data_combination == 2 or data_combination == 3:\n",
    "        # only deepface\n",
    "        data = data_df\n",
    "        labels = labels_df\n",
    "    \n",
    "    if data_combination == 1: # or data_combination == 2:\n",
    "        # only dlib\n",
    "        data, labels  = [], []\n",
    "        for index, dlib_i in enumerate(data_dlib):\n",
    "            if not all(v == 0 for v in dlib_i):\n",
    "                #only if a face is found\n",
    "                data.append(dlib_i) # concatenation of 4096 deepface + 2210 dlib\n",
    "                labels.append(labels_dlib[index])\n",
    "                \n",
    "                \n",
    "    if data_combination == 2 or data_combination == 3 or data_combination == 4:# or data_combination == 3 or data_combination == 4:\n",
    "        # deepface + dlib (all features) \n",
    "        data, labels  = [], []\n",
    "        for index, (df_i, dlib_i) in enumerate(zip(data_df, data_dlib)):\n",
    "            if not all(v == 0 for v in dlib_i):\n",
    "                #only if a face is found \n",
    "                data.append(df_i.tolist()+dlib_i) # concatenation of 4096 deepface + 2210 dlib\n",
    "                labels.append(labels_df[index])\n",
    "                \n",
    "                                               \n",
    "    if data_combination == 3:\n",
    "        # deepface + dlib (x most important features)\n",
    "        # data, labels are already filled from the above if statement\n",
    "                                               \n",
    "        # using a Random Forest the x most important features are used                                   \n",
    "        forest = RandomForestClassifier(n_estimators=10,random_state=0) # 10 has been found with best aroc scores\n",
    "        forest.fit(data, labels)\n",
    "        importances = forest.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        indices = indices[0:nr_feats] \n",
    "\n",
    "        data2 = []\n",
    "        for row in data:\n",
    "            data2.append(np.array(row)[indices])                                \n",
    "        data = data2\n",
    "\n",
    "                                               \n",
    "    nr_comps = 0\n",
    "    if data_combination == 4:\n",
    "        # pca components that explain > 0.9 variance\n",
    "        for i in range(0, np.array(data).shape[0]):\n",
    "            pca = PCA(n_components=i)\n",
    "            components = pca.fit_transform(data)    \n",
    "            if sum(pca.explained_variance_ratio_) > 0.9:\n",
    "                nr_comps = i\n",
    "        \n",
    "        pca = PCA(n_components=nr_comps)\n",
    "        data = pca.fit_transform(data)       \n",
    "        \n",
    "    \n",
    "    if data_combination == 5 or data_combination == 7:\n",
    "        # openface \n",
    "        method = \"openface\"\n",
    "        kdv_csv = data_dir+\"\\\\representations\\kdv-patients-\"+method+\".csv\"  \n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-\"+method+\".csv\"\n",
    "        data_openface, labels_openface = read_rep2(kdv_csv, ID_csv, low_age, high_age, data_dir)\n",
    "\n",
    "        data = []\n",
    "        openface_names = data_openface[:,0]\n",
    "        data_openface = np.array(data_openface)[:, 1:]\n",
    "        for openface_i in data_openface:\n",
    "            rep = [float(i) for i in openface_i.tolist()]\n",
    "            data.append(rep)\n",
    "\n",
    "        labels = np.array(labels_openface)\n",
    "        \n",
    "        \n",
    "    if data_combination == 6 or data_combination == 7:\n",
    "        # cfps\n",
    "        method = \"cfps\"\n",
    "        kdv_csv = data_dir+\"\\\\representations\\kdv-patients-\"+method+\".csv\"  \n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-\"+method+\".csv\"\n",
    "        data_cfps, labels_cfps = read_rep2(kdv_csv, ID_csv, low_age, high_age, data_dir)\n",
    "        \n",
    "        data = []\n",
    "        cfps_names = data_cfps[:,0]\n",
    "        data_cfps = np.array(data_cfps)[:, 1:]\n",
    "        \n",
    "        for cfps_i in data_cfps:\n",
    "            rep = [float(i) for i in cfps_i.tolist()]\n",
    "            data.append(rep)\n",
    "            \n",
    "        labels = np.array(labels_cfps)\n",
    "\n",
    "        \n",
    "    if data_combination == 7:\n",
    "        # openface + cfps \n",
    "           \n",
    "        matches = [i==j for i, j in zip(openface_names, cfps_names)]\n",
    "\n",
    "        data, labels  = [], []\n",
    "        if False not in matches:\n",
    "            for index, (openface_i, cfps_i) in enumerate(zip(data_openface, data_cfps)):\n",
    "                rep_list = openface_i.tolist()+cfps_i.tolist()\n",
    "                rep = [float(i) for i in rep_list]\n",
    "                data.append(rep) # concatenation of 128 openface + 340 cfps\n",
    "                labels.append(labels_openface[index].astype(np.float64))\n",
    "        else:\n",
    "            print(\"Not the same image names for openface and cfps representation.\")\n",
    "\n",
    "    if data_combination == 8:\n",
    "        # facereader\n",
    "        method = \"facereader-landmarks\"\n",
    "        kdv_csv = data_dir+\"\\\\representations\\kdv-patients-\"+method+\".csv\"  \n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-\"+method+\".csv\"\n",
    "\n",
    "        data_fr, labels_fr = read_rep_fr(kdv_csv, ID_csv, low_age, high_age, data_dir)      \n",
    "        \n",
    "        data, labels  = [], []\n",
    "        for index, fr_i in enumerate(data_fr):\n",
    "            if not all(v == 0 for v in fr_i):\n",
    "                data.append(fr_i)\n",
    "                labels.append(labels_fr[index])\n",
    "    \n",
    "    return 0, np.array(data), np.array(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header(data_combination, nr_feats):\n",
    "    if data_combination == 0:\n",
    "        return \"0: Classifying data with deepface representation\\n\\n\"\n",
    "        \n",
    "    if data_combination == 1:\n",
    "        return\"1: Classifying data with dlib representation\\n\\n\"\n",
    "            \n",
    "    if data_combination == 2:\n",
    "        return \"2: Classifying data with all deepface+dlib representations\\n\\n\"\n",
    "            \n",
    "    if data_combination == 3:\n",
    "        return \"3: Classifying data with the {} most important features of deepface-dlib representations\\n\\n\".format(nr_feats)\n",
    "        \n",
    "    if data_combination == 4:\n",
    "        return \"4: Classifying data with PCA components of deepface-dlib representation\\n\"\n",
    "    \n",
    "    if data_combination == 5:\n",
    "        return \"5: Classifying data with openface representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 6:\n",
    "        return \"6: Classifying data with cfps representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 7:\n",
    "        return \"7: Classifying data with openface+cfps representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 8:\n",
    "        return \"8: Classifying data with facereader representation\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier_conf_matrix(data, labels, k, norm):\n",
    "\n",
    "            \n",
    "    data = normalize(data, norm) \n",
    "    all_y, all_probs, all_preds = [], [], [] \n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # leave one out split and make prediction\n",
    "    for train, test in loo.split(data):\n",
    "        all_y.append(labels[test])\n",
    "        model = SVC(kernel=k, probability=True)\n",
    "        model = model.fit(data[train], labels[train])\n",
    "        all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "        all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "    # based on all predictions make aroc curve and confusion matrix\n",
    "    aroc = roc_auc_score(all_y, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn)\n",
    "\n",
    "                \n",
    "    return tn, fp, fn, tp, aroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(tn, fp, fn, tp, aroc):\n",
    "    \n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn)\n",
    "\n",
    "    print(\"Aroc: {}\".format(aroc))\n",
    "    print(\"Specificity: {}\".format(spec))\n",
    "    print(\"Sensitivity: {}\".format(sens))\n",
    "    \n",
    "    \n",
    "    conf_matrix = [[tp, fp],\n",
    "             [fn, tn]]\n",
    "    df_cm = pd.DataFrame(conf_matrix, index = [\"ADNP_pred\", \"Control_pred\"],\n",
    "                      columns = [\"ADNP\", \"Control\"])\n",
    "    plt.figure(figsize = (6, 6))\n",
    "    sns_heat = sns.heatmap(df_cm, annot=True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (53, 6306) and labels shape: (53,) \n",
      "\n",
      "\n",
      "SVM classifier with a linear kernel and normalizer is Standardscaler (z-scores)\n",
      "Aroc: 0.9515669515669516\n",
      "Specificity: 0.9629629629629629\n",
      "Sensitivity: 0.8846153846153846\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done running main file\n"
     ]
    }
   ],
   "source": [
    "syn_name = \"ADNP\"\n",
    "\n",
    "def main():    \n",
    "\n",
    "    today = date.today()\n",
    "    start = time.time()\n",
    "    \n",
    "    data_dir = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\{}\".format(syn_name) \n",
    "    #results_file = open(\"results/{}-results-{}.txt\".format(syn_name, today), \"w\")\n",
    "\n",
    "    nr_feats = 300\n",
    "    \n",
    "\n",
    "    \n",
    "    for data_combination in [2]: # 0, 1, 2, 3]: # [0, 1, 2, 3, 4, 5, 6, 7, 8]: rest works\n",
    "        \n",
    "        #results_file.write(get_header(data_combination, nr_feats))\n",
    "        print(get_header(data_combination, nr_feats))            \n",
    "\n",
    "        # data, labels depend on data_combination\n",
    "        nr_comps, data, labels = concatenate(data_dir, data_combination, nr_feats) \n",
    "\n",
    "        print(\"Data shape: {} and labels shape: {} \\n\\n\".format(data.shape, labels.shape))\n",
    "\n",
    "        if data_combination == 4:\n",
    "            results_file.write(\"Nr of pca components used: {}\\n\\n\".format(nr_comps))\n",
    "        \n",
    "#         ## conf matrix - ADNP ##\n",
    "#         print(\"SVM classifier with a linear kernel and normalizer is Standardscaler (z-scores)\")\n",
    "#         tn, fp, fn, tp, aroc  = svm_classifier_conf_matrix(data, labels, 'linear', 2)\n",
    "#         plot_conf_matrix(tn, fp, fn, tp, aroc)\n",
    "\n",
    "        # plot representation\n",
    "        # plot_pca_tsne(data, labels, low_age, high_age)\n",
    "\n",
    "#         results_file.write(\"CLASSIFIER RESULTS for {} patients and controls \\n\".format(syn_name))\n",
    "\n",
    "#         k, knn_norm, knn_aroc, knn_spec, knn_sens = knn_classifier(data, labels)\n",
    "#         results_file.write(\"knn classifier (k = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(k, knn_norm, knn_aroc, knn_spec, knn_sens))\n",
    "\n",
    "#         kernel, svm_norm, svm_aroc, svm_spec, svm_sens = svm_classifier(data, labels)\n",
    "#         results_file.write(\"svm classifier (k = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(kernel, svm_norm, svm_aroc, svm_spec, svm_sens))\n",
    "\n",
    "#         n_trees_rf, rf_norm, rf_aroc, rf_spec, rf_sens = rf_classifier(data, labels)\n",
    "#         results_file.write(\"Random Forest classifier (trees = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(n_trees_rf, rf_norm, rf_aroc, rf_spec, rf_sens))\n",
    "\n",
    "#         n_trees_gr, gr_norm, gr_aroc, gr_spec, gr_sens = gr_classifier(data, labels)\n",
    "#         results_file.write(\"Gradient Boost classifier (trees = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(n_trees_gr, gr_norm, gr_aroc, gr_spec, gr_sens))\n",
    "\n",
    "#         n_trees_ada, ada_norm, ada_aroc, ada_spec, ada_sens = ada_classifier(data, labels)\n",
    "#         results_file.write(\"Ada Boost classifier (trees = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(n_trees_ada, ada_norm, ada_aroc, ada_spec, ada_sens))\n",
    "\n",
    "#         results_file.write(\"\\n\")\n",
    "\n",
    "#     end = time.time()\n",
    "#     results_file.write(\"Running this whole file took {:.2f} hours\".format((end-start)/3600.00))\n",
    "#     results_file.close()\n",
    "    print(\"done running main file\")\n",
    "    \n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 7: VotingClassifier with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/rmferg/soft-voting-classifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1, random_state=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "t0 = time.clock()\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=1)\n",
    "svm = SVC(probability=True, kernel='rbf')\n",
    "knn = KNeighborsClassifier(p=2, metric='minkowski')\n",
    "nb = GaussianNB()\n",
    "eclf = VotingClassifier(estimators=[('tree', tree), ('svm', svm), ('knn', knn),('nb', nb)], voting='soft')\n",
    "\n",
    "param_range10 = [.001, .01, 1, 10, 100]\n",
    "param_range1 = list(range(3, 8))\n",
    "param_grid = [{'svm__C':param_range10, 'svm__gamma':param_range10, 'tree__max_depth':param_range1, \n",
    "               'knn__n_neighbors':param_range1}]\n",
    "\n",
    "gs = GridSearchCV(estimator=eclf, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "gs = gs.fit(X_train_std, y_train)\n",
    "\n",
    "print('Best accuracy score: %.3f \\nBest parameters: %s' % (gs.best_score_, gs.best_params_))\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train_std, y_train)\n",
    "t1 = time.clock()\n",
    "print('Running time: %.3f' % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test_std)\n",
    "print('ROC AUC: %.3f \\nAccuracy: %.3f \\nConfusion Matrix:' % (roc_auc_score(y_true=y_test, y_score=y_pred),\n",
    "                                         accuracy_score(y_true=y_test, y_pred=y_pred)))\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 7: Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_classifiers(k, kernel, n_trees_rf, n_trees_gr, n_trees_ada, data, labels):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    svm = SVC(kernel=kernel, probability=True)\n",
    "    random_forest = RandomForestClassifier(n_estimators=n_trees_rf) \n",
    "    gr_clf = GradientBoostingClassifier(n_estimators=n_trees_gr, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    ada_clf = AdaBoostClassifier(n_estimators=n_trees_ada, random_state=0)\n",
    "    nb = GaussianNB()\n",
    "\n",
    "    classifiers = [('knn', knn), ('svm', svm),('gr_clf', gr_clf), ('random_forest', random_forest), ('nb', nb), ('ada_clf', ada_clf)]\n",
    "    \n",
    "    cv = int(len(labels)/2)\n",
    "\n",
    "    classifier_combinations = []\n",
    "    for L in range(1, len(classifiers)+1):\n",
    "        for subset in itertools.combinations(classifiers, L):\n",
    "            classifier_combinations.append(subset)\n",
    "    \n",
    "    best_ensemble_acc = 0\n",
    "    best_classifier_com = None\n",
    "    best_norm = -1\n",
    "    best_vote = None\n",
    "\n",
    "    for classifier_com in classifier_combinations:\n",
    "    #for classifier_com in tqdm(classifier_combinations):\n",
    "        \n",
    "        for normalize in [0, 1, 2]:\n",
    "            for vote_sort in ['soft', 'hard']:\n",
    "\n",
    "                weights = np.ones(len(classifier_com)).tolist()\n",
    "                ensemble_clf = VotingClassifier(estimators=classifier_com,\n",
    "                                        voting=vote_sort,\n",
    "                                        weights=weights)\n",
    "                \n",
    "                            \n",
    "                mean_acc = cross_val_classifier(ensemble_clf, data, labels, normalize)\n",
    "                \n",
    "                clf_string = \"\"\n",
    "                for clf in classifier_com:\n",
    "                    clf_string  = clf_string + \" \" + clf[0]\n",
    "                    \n",
    "                print(\"Ensemble: {}\".format(clf_string))\n",
    "                print(\"With accuracy: {}\".format(mean_acc))\n",
    "                \n",
    "                if mean_acc > best_ensemble_acc:\n",
    "                    best_ensemble_acc = mean_acc\n",
    "                    best_norm = normalize\n",
    "                    best_vote = vote_sort\n",
    "                    best_classifier_com = classifier_com\n",
    "    \n",
    "    return best_ensemble_acc, best_classifier_com, best_vote, best_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
