{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different classifiers on data of KdV patients and ID controls\n",
    "Overview of this notebook:\n",
    "\n",
    "First the deepface representations of the cropped images are read in from an Excel file. The data is then plotted by using either t-sne or PCA for dimension reduction. It is clear that there aren't two clear clusters.\n",
    "\n",
    "In the rest of the notebook the following classifiers are tested: k-NN, SVM, Random Forest, Gradient Boosting, AdaBoost, Gaussian Naive Bayes. In the end also an ensemble of all these methods or some of them is tried. None outperforming the Gradient Boosting classifier. \n",
    "\n",
    "To normalize the data either Normalizer (unit form) or StandardScaler (z = (x - mean)/std) is used, without any specific difference in performance yet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rep(kdv_csv, ID_csv, low_age, high_age, data_dir):\n",
    "    \n",
    "    # open directories\n",
    "    kdv_dir = data_dir+\"\\\\kdv-patients-age-group-\"+str(low_age) + \"-\" + str(high_age)\n",
    "    ID_dir = data_dir+ \"\\\\kdv-selected-ID-controls-age-group-\"+str(low_age) + \"-\" + str(high_age)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_kdv = [f for f in listdir(kdv_dir) if (isfile(join(kdv_dir, f)) & (\"crop_sized.jpg\" in f))]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f)) & (\"crop_sized.JPG\" in f))]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, csv_file in enumerate([ID_csv, kdv_csv]):\n",
    "        with open (csv_file, newline='') as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if row[0] in files_kdv or row[0] in files_ID:\n",
    "                    rep = list(map(float, row[1:]))\n",
    "                    data.append(rep)\n",
    "                    labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rep2(kdv_csv, ID_csv, low_age, high_age, data_dir):\n",
    "    \n",
    "    # open directories\n",
    "    kdv_dir = data_dir+\"\\\\kdv-patients-age-group-\"+str(low_age) + \"-\" + str(high_age)\n",
    "    ID_dir = data_dir+ \"\\\\kdv-selected-ID-controls-age-group-\"+str(low_age) + \"-\" + str(high_age)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_kdv = [f for f in listdir(kdv_dir) if (isfile(join(kdv_dir, f)) & (\"crop_sized.jpg\" in f))]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f)) & (\"crop_sized.JPG\" in f))]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, csv_file in enumerate([ID_csv, kdv_csv]):\n",
    "        with open (csv_file, newline='') as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if row[0] in files_kdv or row[0] in files_ID:\n",
    "                    rep = list(map(float, row[1:]))\n",
    "                    data.append(row)\n",
    "                    labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_tsne(data, labels, lowest_age = -1, highest_age = -1):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot([1,2])\n",
    "\n",
    "    # visualize data in tnse (men/women)\n",
    "    X_embedded_tsne = TSNE(n_components=2, init='pca').fit_transform(data)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    unique = list(set(labels))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [X_embedded_tsne[j, 0] for j  in range(len(X_embedded_tsne[:,0])) if labels[j] == u]\n",
    "        yi = [X_embedded_tsne[j, 1] for j  in range(len(X_embedded_tsne[:,1])) if labels[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors[i]], label=str(u))\n",
    "    plt.legend()\n",
    "    plt.title(\"t-sne for age range {}-{}\".format(lowest_age, highest_age))\n",
    "\n",
    "    # visualize data in pca (men/women)\n",
    "    X_embedded_pca = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    unique = list(set(labels))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [X_embedded_pca[j, 0] for j  in range(len(X_embedded_pca[:,0])) if labels[j] == u]\n",
    "        yi = [X_embedded_pca[j, 1] for j  in range(len(X_embedded_pca[:,1])) if labels[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors[i]], label=str(u))\n",
    "    plt.legend()\n",
    "    plt.title(\"pca for age range{}-{}\".format(lowest_age, highest_age))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred): \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    plt.figure(1, figsize=(12,6))\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='LOOCV ROC (AUC = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Chance level', alpha=.8)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def calculate_metrics_loo(model, data, labels):\n",
    "    all_y, all_probs, all_preds = [], [], [] \n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    for train, test in loo.split(data):\n",
    "        all_y.append(labels[test])\n",
    "        model = model.fit(data[train], labels[train])\n",
    "        all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "        all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "        \n",
    "    aroc = roc_auc_score(all_y, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn) \n",
    "    \n",
    "    return aroc, spec, sens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, i):\n",
    "\n",
    "    if i == 0:\n",
    "        return data\n",
    "    \n",
    "    if i == 1:\n",
    "        return Normalizer().fit_transform(data)\n",
    "        \n",
    "    if i == 2:\n",
    "        return StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(data, labels):\n",
    "    k_values = [3,5,7,9,11,13,15]\n",
    "    best_aroc = 0\n",
    "    best_k = 0\n",
    "\n",
    "    for k in tqdm(k_values):\n",
    "        for i in [0, 1, 2]:\n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_k = k\n",
    "                \n",
    "    return best_k, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(data, labels):\n",
    "    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    best_aroc = 0\n",
    "    best_kernel = None\n",
    "\n",
    "    for k in tqdm(kernels):\n",
    "        for i in [0, 1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = SVC(kernel=k, probability=True)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_kernel = k\n",
    "                \n",
    "    return best_kernel, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20] #, 40, 60, 80]\n",
    "    best_estimator_rf = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for i in [0, 1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = RandomForestClassifier(n_estimators=est)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_estimator_rf = est\n",
    "    \n",
    "    return best_estimator_rf, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20] #, 40, 60, 80]\n",
    "    best_estimator_gr = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for i in [0, 1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = GradientBoostingClassifier(n_estimators=est)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_estimator_gr = est\n",
    "                \n",
    "            if best_aroc > 0.8:\n",
    "                print(\"tn {}, fp {}, fn {}, tp {}\".format(tn, fp, fn, tp))\n",
    "                print(\"aroc: {} , spec: {}, sens: {}\".format(best_aroc, spec, sens))\n",
    "                print(\"trees: {}, norm: {}\".format(best_estimator_gr, best_norm))\n",
    "                conf_matrix = [[tp, fp],\n",
    "                             [fn, tn]]\n",
    "                df_cm = pd.DataFrame(conf_matrix, index = [\"Kdvs_pred\", \"Control_pred\"], columns = [\"Kdvs\", \"Control\"])\n",
    "                plt.figure(figsize = (6, 6))\n",
    "                sns_heat = sns.heatmap(df_cm, annot=True)\n",
    "                plt.show()                \n",
    "                \n",
    "                                \n",
    "    return best_estimator_gr, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20 ] #, 40, 60, 80]\n",
    "    best_estimator_ada = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for i in [0,1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = AdaBoostClassifier(n_estimators=est)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_estimator_ada = est\n",
    "                \n",
    "    return best_estimator_ada, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(low_age, high_age, data_dir, data_combination, nr_feats): #data_df, labels_df, data_dlib, labels_dlib, data_combination, nr_feats):\n",
    "#  nr_comps, data, labels = concatenate(data_dir, data_combination, nr_feats) #data_df, labels_df, data_dlib, labels_dlib, data_combination, nr_feats)\n",
    "\n",
    "#     # assert data has same shape and labels are exactly the same\n",
    "#     assert data_df.shape[0] == data_dlib.shape[0]\n",
    "#     assert labels_df.shape == labels_dlib.shape\n",
    "#     match = [True for i, j in zip(labels_df, labels_dlib) if i == j]\n",
    "#     assert False not in match\n",
    "\n",
    "    method = \"deepface\"\n",
    "    kdv_csv = data_dir+\"\\\\representations\\kdv-patients-\"+method+\".csv\"  \n",
    "    ID_csv  = data_dir+\"\\\\representations\\ID-controls-\"+method+\".csv\"\n",
    "    data_df, labels_df = read_rep(kdv_csv, ID_csv, low_age, high_age, data_dir)\n",
    "    \n",
    "    method = \"dlib\"\n",
    "    kdv_csv = data_dir+\"\\\\representations\\kdv-patients-\"+method+\".csv\"  \n",
    "    ID_csv  = data_dir+\"\\\\representations\\ID-controls-\"+method+\".csv\"\n",
    "    data_dlib, labels_dlib = read_rep(kdv_csv, ID_csv, low_age, high_age, data_dir)\n",
    "\n",
    "    \n",
    "    if data_combination == 0: # or data_combination == 2 or data_combination == 3:\n",
    "        # only deepface\n",
    "        data = data_df\n",
    "        labels = labels_df\n",
    "    \n",
    "    if data_combination == 1: # or data_combination == 2:\n",
    "        # only dlib\n",
    "        data, labels  = [], []\n",
    "        for index, dlib_i in enumerate(data_dlib):\n",
    "            if not all(v == 0 for v in dlib_i):\n",
    "                #only if a face is found\n",
    "                data.append(dlib_i) # concatenation of 4096 deepface + 2210 dlib\n",
    "                labels.append(labels_dlib[index])\n",
    "                \n",
    "                \n",
    "    if data_combination == 2 or data_combination == 3 or data_combination == 4:# or data_combination == 3 or data_combination == 4:\n",
    "        # deepface + dlib (all features) \n",
    "        data, labels  = [], []\n",
    "        for index, (df_i, dlib_i) in enumerate(zip(data_df, data_dlib)):\n",
    "            if not all(v == 0 for v in dlib_i):\n",
    "                #only if a face is found \n",
    "                data.append(df_i.tolist()+dlib_i) # concatenation of 4096 deepface + 2210 dlib\n",
    "                labels.append(labels_df[index])\n",
    "                \n",
    "                                               \n",
    "    if data_combination == 3:\n",
    "        # deepface + dlib (x most important features)\n",
    "        # data, labels are already filled from the above if statement\n",
    "                                               \n",
    "        # using a Random Forest the x most important features are used                                   \n",
    "        forest = RandomForestClassifier(n_estimators=10,random_state=0) # 10 has been found with best aroc scores\n",
    "        forest.fit(data, labels)\n",
    "        importances = forest.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        indices = indices[0:nr_feats] \n",
    "\n",
    "        data2 = []\n",
    "        for row in data:\n",
    "            data2.append(np.array(row)[indices])                                \n",
    "        data = data2\n",
    "\n",
    "                                               \n",
    "    nr_comps = 0\n",
    "    if data_combination == 4:\n",
    "        # pca components that explain > 0.9 variance\n",
    "        for i in range(0, np.array(data).shape[0]):\n",
    "            pca = PCA(n_components=i)\n",
    "            components = pca.fit_transform(data)    \n",
    "            if sum(pca.explained_variance_ratio_) > 0.9:\n",
    "                nr_comps = i\n",
    "        \n",
    "        pca = PCA(n_components=nr_comps)\n",
    "        data = pca.fit_transform(data)       \n",
    "        \n",
    "    \n",
    "    if data_combination == 5 or data_combination == 7:\n",
    "        # openface \n",
    "        method = \"openface\"\n",
    "        kdv_csv = data_dir+\"\\\\representations\\kdv-patients-\"+method+\".csv\"  \n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-\"+method+\".csv\"\n",
    "        data_openface, labels_openface = read_rep2(kdv_csv, ID_csv, low_age, high_age, data_dir)\n",
    "\n",
    "        data = []\n",
    "        openface_names = data_openface[:,0]\n",
    "        data_openface = np.array(data_openface)[:, 1:]\n",
    "        for openface_i in data_openface:\n",
    "            rep = [float(i) for i in openface_i.tolist()]\n",
    "            data.append(rep)\n",
    "\n",
    "        labels = np.array(labels_openface)\n",
    "        \n",
    "        \n",
    "    if data_combination == 6 or data_combination == 7:\n",
    "        # cfps\n",
    "        method = \"cfps\"\n",
    "        kdv_csv = data_dir+\"\\\\representations\\kdv-patients-\"+method+\".csv\"  \n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-\"+method+\".csv\"\n",
    "        data_cfps, labels_cfps = read_rep2(kdv_csv, ID_csv, low_age, high_age, data_dir)\n",
    "        \n",
    "        data = []\n",
    "        cfps_names = data_cfps[:,0]\n",
    "        data_cfps = np.array(data_cfps)[:, 1:]\n",
    "        \n",
    "        for cfps_i in data_cfps:\n",
    "            rep = [float(i) for i in cfps_i.tolist()]\n",
    "            data.append(rep)\n",
    "            \n",
    "        labels = np.array(labels_cfps)\n",
    "\n",
    "        \n",
    "    if data_combination == 7:\n",
    "        # openface + cfps \n",
    "           \n",
    "        matches = [i==j for i, j in zip(openface_names, cfps_names)]\n",
    "\n",
    "        data, labels  = [], []\n",
    "        if False not in matches:\n",
    "            for index, (openface_i, cfps_i) in enumerate(zip(data_openface, data_cfps)):\n",
    "                rep_list = openface_i.tolist()+cfps_i.tolist()\n",
    "                rep = [float(i) for i in rep_list]\n",
    "                data.append(rep) # concatenation of 128 openface + 340 cfps\n",
    "                labels.append(labels_openface[index].astype(np.float64))\n",
    "        else:\n",
    "            print(\"Not the same image names for openface and cfps representation.\")\n",
    "\n",
    "    if data_combination == 8:\n",
    "        # facereader\n",
    "        method = \"facereader\"\n",
    "        kdv_csv = data_dir+\"\\\\representations\\kdv-patients-\"+method+\".csv\"  \n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-\"+method+\".csv\"\n",
    "\n",
    "        data_fr, labels_fr = read_rep(kdv_csv, ID_csv, low_age, high_age, data_dir)\n",
    "        \n",
    "        data, labels  = [], []\n",
    "        for index, fr_i in enumerate(data_fr):\n",
    "            if not all(v == 0 for v in fr_i):\n",
    "                data.append(fr_i)\n",
    "                labels.append(labels_fr[index])\n",
    "                \n",
    "    \n",
    "    return 0, np.array(data), np.array(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header(data_combination, nr_feats):\n",
    "    if data_combination == 0:\n",
    "        return \"0: Classifying data with deepface representation\\n\\n\"\n",
    "        \n",
    "    if data_combination == 1:\n",
    "        return\"1: Classifying data with dlib representation\\n\\n\"\n",
    "            \n",
    "    if data_combination == 2:\n",
    "        return \"2: Classifying data with all deepface+dlib representations\\n\\n\"\n",
    "            \n",
    "    if data_combination == 3:\n",
    "        return \"3: Classifying data with the {} most important features of deepface-dlib representations\\n\\n\".format(nr_feats)\n",
    "        \n",
    "    if data_combination == 4:\n",
    "        return \"4: Classifying data with PCA components of deepface-dlib representation\\n\"\n",
    "    \n",
    "    if data_combination == 5:\n",
    "        return \"5: Classifying data with openface representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 6:\n",
    "        return \"6: Classifying data with cfps representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 7:\n",
    "        return \"7: Classifying data with openface+cfps representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 8:\n",
    "        return \"8: Classifying data with facereader representation\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (140, 4096) and labels shape: (140,) \n",
      "\n",
      "\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (136, 2210) and labels shape: (136,) \n",
      "\n",
      "\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (136, 6306) and labels shape: (136,) \n",
      "\n",
      "\n",
      "3: Classifying data with the 300 most important features of deepface-dlib representations\n",
      "\n",
      "\n",
      "Data shape: (136, 300) and labels shape: (136,) \n",
      "\n",
      "\n",
      "4: Classifying data with PCA components of deepface-dlib representation\n",
      "\n",
      "Data shape: (136, 135) and labels shape: (136,) \n",
      "\n",
      "\n",
      "5: Classifying data with openface representation\n",
      "\n",
      "\n",
      "Data shape: (134, 128) and labels shape: (134,) \n",
      "\n",
      "\n",
      "6: Classifying data with cfps representation\n",
      "\n",
      "\n",
      "Data shape: (134, 340) and labels shape: (134,) \n",
      "\n",
      "\n",
      "7: Classifying data with openface+cfps representation\n",
      "\n",
      "\n",
      "Data shape: (134, 468) and labels shape: (134,) \n",
      "\n",
      "\n",
      "8: Classifying data with facereader representation\n",
      "\n",
      "\n",
      "Data shape: (0,) and labels shape: (0,) \n",
      "\n",
      "\n",
      "done running main file\n"
     ]
    }
   ],
   "source": [
    "def main():    \n",
    "    \n",
    "    today = date.today()\n",
    "    start = time.time()\n",
    "    \n",
    "    data_dir = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\kdv\" \n",
    "    results_file = open(\"results/kdv-results-\" + str(today)+\".txt\", \"w\")\n",
    "\n",
    "    nr_feats = 300\n",
    "    \n",
    "    \n",
    "    for data_combination in [8]: # [0, 1, 2, 3, 4, 5, 6, 7, 8]: rest works\n",
    "        \n",
    "        results_file.write(get_header(data_combination, nr_feats))\n",
    "        print(get_header(data_combination, nr_feats))\n",
    "            \n",
    "        age_ranges = [[1, 40]] #[[1, 3], [4, 16], [17, 40]]  \n",
    "\n",
    "        for [low_age, high_age] in age_ranges:\n",
    "            \n",
    "\n",
    "            \n",
    "            # data, labels depend on data_combination\n",
    "            nr_comps, data, labels = concatenate(low_age, high_age, data_dir, data_combination, nr_feats) \n",
    "            \n",
    "            print(\"Data shape: {} and labels shape: {} \\n\\n\".format(data.shape, labels.shape))\n",
    "\n",
    "            if data_combination == 4:\n",
    "                results_file.write(\"Nr of pca components used: {}\\n\\n\".format(nr_comps))\n",
    "\n",
    "\n",
    "            # plot representation\n",
    "            # plot_pca_tsne(data, labels, low_age, high_age)\n",
    "\n",
    "            results_file.write(\"CLASSIFIER RESULTS for kdv-control age group \" + str(low_age) + \"-\" + str(high_age) + \"\\n\")\n",
    "\n",
    "            #k, knn_norm, knn_aroc, knn_spec, knn_sens = knn_classifier(data, labels)\n",
    "            #results_file.write(\"knn classifier (k = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(k, knn_norm, knn_aroc, knn_spec, knn_sens))\n",
    "            \n",
    "            #kernel, svm_norm, svm_aroc, svm_spec, svm_sens = svm_classifier(data, labels)\n",
    "            #results_file.write(\"svm classifier (k = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(kernel, svm_norm, svm_aroc, svm_spec, svm_sens))\n",
    "\n",
    "            #n_trees_rf, rf_norm, rf_aroc, rf_spec, rf_sens = rf_classifier(data, labels)\n",
    "            #results_file.write(\"Random Forest classifier (trees = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(n_trees_rf, rf_norm, rf_aroc, rf_spec, rf_sens))\n",
    "\n",
    "            #n_trees_gr, gr_norm, gr_aroc, gr_spec, gr_sens = gr_classifier(data, labels)\n",
    "            #results_file.write(\"Gradient Boost classifier (trees = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(n_trees_gr, gr_norm, gr_aroc, gr_spec, gr_sens))\n",
    "\n",
    "            #n_trees_ada, ada_norm, ada_aroc, ada_spec, ada_sens = ada_classifier(data, labels)\n",
    "            #results_file.write(\"Ada Boost classifier (trees = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(n_trees_ada, ada_norm, ada_aroc, ada_spec, ada_sens))\n",
    "\n",
    "            results_file.write(\"\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    results_file.write(\"Running this whole file took {:.2f} hours\".format((end-start)/3600.00))\n",
    "    results_file.close()\n",
    "    print(\"done running main file\")\n",
    "    \n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 7: VotingClassifier with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/rmferg/soft-voting-classifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1, random_state=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "t0 = time.clock()\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=1)\n",
    "svm = SVC(probability=True, kernel='rbf')\n",
    "knn = KNeighborsClassifier(p=2, metric='minkowski')\n",
    "nb = GaussianNB()\n",
    "eclf = VotingClassifier(estimators=[('tree', tree), ('svm', svm), ('knn', knn),('nb', nb)], voting='soft')\n",
    "\n",
    "param_range10 = [.001, .01, 1, 10, 100]\n",
    "param_range1 = list(range(3, 8))\n",
    "param_grid = [{'svm__C':param_range10, 'svm__gamma':param_range10, 'tree__max_depth':param_range1, \n",
    "               'knn__n_neighbors':param_range1}]\n",
    "\n",
    "gs = GridSearchCV(estimator=eclf, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "gs = gs.fit(X_train_std, y_train)\n",
    "\n",
    "print('Best accuracy score: %.3f \\nBest parameters: %s' % (gs.best_score_, gs.best_params_))\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train_std, y_train)\n",
    "t1 = time.clock()\n",
    "print('Running time: %.3f' % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test_std)\n",
    "print('ROC AUC: %.3f \\nAccuracy: %.3f \\nConfusion Matrix:' % (roc_auc_score(y_true=y_test, y_score=y_pred),\n",
    "                                         accuracy_score(y_true=y_test, y_pred=y_pred)))\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 7: Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_classifiers(k, kernel, n_trees_rf, n_trees_gr, n_trees_ada, data, labels):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    svm = SVC(kernel=kernel, probability=True)\n",
    "    random_forest = RandomForestClassifier(n_estimators=n_trees_rf) \n",
    "    gr_clf = GradientBoostingClassifier(n_estimators=n_trees_gr, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    ada_clf = AdaBoostClassifier(n_estimators=n_trees_ada, random_state=0)\n",
    "    nb = GaussianNB()\n",
    "\n",
    "    classifiers = [('knn', knn), ('svm', svm),('gr_clf', gr_clf), ('random_forest', random_forest), ('nb', nb), ('ada_clf', ada_clf)]\n",
    "    \n",
    "    cv = int(len(labels)/2)\n",
    "\n",
    "    classifier_combinations = []\n",
    "    for L in range(1, len(classifiers)+1):\n",
    "        for subset in itertools.combinations(classifiers, L):\n",
    "            classifier_combinations.append(subset)\n",
    "    \n",
    "    best_ensemble_acc = 0\n",
    "    best_classifier_com = None\n",
    "    best_norm = -1\n",
    "    best_vote = None\n",
    "\n",
    "    for classifier_com in classifier_combinations:\n",
    "    #for classifier_com in tqdm(classifier_combinations):\n",
    "        \n",
    "        for normalize in [0, 1, 2]:\n",
    "            for vote_sort in ['soft', 'hard']:\n",
    "\n",
    "                weights = np.ones(len(classifier_com)).tolist()\n",
    "                ensemble_clf = VotingClassifier(estimators=classifier_com,\n",
    "                                        voting=vote_sort,\n",
    "                                        weights=weights)\n",
    "                \n",
    "                            \n",
    "                mean_acc = cross_val_classifier(ensemble_clf, data, labels, normalize)\n",
    "                \n",
    "                clf_string = \"\"\n",
    "                for clf in classifier_com:\n",
    "                    clf_string  = clf_string + \" \" + clf[0]\n",
    "                    \n",
    "                print(\"Ensemble: {}\".format(clf_string))\n",
    "                print(\"With accuracy: {}\".format(mean_acc))\n",
    "                \n",
    "                if mean_acc > best_ensemble_acc:\n",
    "                    best_ensemble_acc = mean_acc\n",
    "                    best_norm = normalize\n",
    "                    best_vote = vote_sort\n",
    "                    best_classifier_com = classifier_com\n",
    "    \n",
    "    return best_ensemble_acc, best_classifier_com, best_vote, best_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
