{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import argparse\n",
    "import cv2\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "import openface\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument parsing and loading libraries took 0.241365909576 seconds.\n",
      "Loading the dlib and OpenFace models took 3.3763730526 seconds.\n"
     ]
    }
   ],
   "source": [
    "# From https://github.com/cmusatyalab/openface/blob/master/demos/compare.py\n",
    "\n",
    "imgs = [] # input images\n",
    "dlibFacePredictor = \"/home/fien/openface/models/dlib/shape_predictor_68_face_landmarks.dat\"\n",
    "networkModel = \"/home/fien/openface/models/openface/nn4.small2.v1.t7\"\n",
    "imgDim = 96\n",
    "verbose = True\n",
    "\n",
    "if verbose:\n",
    "    print(\"Argument parsing and loading libraries took {} seconds.\".format(\n",
    "        time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "align = openface.AlignDlib(dlibFacePredictor)\n",
    "net = openface.TorchNeuralNet(networkModel, imgDim)\n",
    "   \n",
    "if verbose:\n",
    "    print(\"Loading the dlib and OpenFace models took {} seconds.\".format(\n",
    "        time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRep(imgPath):\n",
    "    if verbose:\n",
    "        print(\"Processing {}.\".format(imgPath))\n",
    "    bgrImg = cv2.imread(imgPath)\n",
    "    if bgrImg is None:\n",
    "        raise Exception(\"Unable to load image: {}\".format(imgPath))\n",
    "    rgbImg = cv2.cvtColor(bgrImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"  + Original size: {}\".format(rgbImg.shape))\n",
    "\n",
    "    start = time.time()\n",
    "    bb = align.getLargestFaceBoundingBox(rgbImg)\n",
    "    if bb is None:\n",
    "        raise Exception(\"Unable to find a face: {}\".format(imgPath))\n",
    "    if verbose:\n",
    "        print(\"  + Face detection took {} seconds.\".format(time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    alignedFace = align.align(imgDim, rgbImg, bb,\n",
    "                              landmarkIndices=openface.AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "    if alignedFace is None:\n",
    "        raise Exception(\"Unable to align image: {}\".format(imgPath))\n",
    "    if verbose:\n",
    "        print(\"  + Face alignment took {} seconds.\".format(time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    rep = net.forward(alignedFace) \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  + OpenFace forward pass took {} seconds.\".format(time.time() - start))\n",
    "        print(\"Representation:\")\n",
    "        print(rep)\n",
    "        print(\" Length of the representation \" + str(len(rep)) + \" and type \" + str(type(rep)))\n",
    "        print(\"-----\\n\")\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing faces/face1.jpg.\n",
      "  + Original size: (512, 512, 3)\n",
      "  + Face detection took 0.274261951447 seconds.\n",
      "  + Face alignment took 0.0157899856567 seconds.\n",
      "  + OpenFace forward pass took 0.119620084763 seconds.\n",
      "Representation:\n",
      "[ 0.07 -0.11  0.04  0.   -0.02  0.03 -0.    0.06 -0.02 -0.12  0.1   0.1\n",
      "  0.07 -0.08 -0.07  0.08 -0.19 -0.13 -0.19  0.17  0.07 -0.03  0.08 -0.05\n",
      " -0.12  0.06  0.03 -0.04 -0.07 -0.06 -0.1   0.08  0.02  0.18 -0.16  0.02\n",
      " -0.04  0.09 -0.06  0.07  0.05 -0.16 -0.02 -0.04 -0.04 -0.08  0.02 -0.03\n",
      " -0.07  0.2  -0.02 -0.06 -0.16  0.16  0.1  -0.08 -0.03 -0.04 -0.11  0.1\n",
      " -0.04 -0.04  0.06  0.03  0.02 -0.01 -0.05  0.03 -0.01  0.03 -0.09 -0.03\n",
      " -0.    0.15  0.11 -0.03  0.11 -0.04 -0.05  0.1   0.07 -0.19  0.03 -0.02\n",
      " -0.14  0.13  0.09  0.03 -0.02  0.08  0.11 -0.15 -0.1   0.09 -0.12  0.07\n",
      " -0.08 -0.03  0.08  0.02 -0.01 -0.06 -0.08  0.07  0.15  0.04 -0.06  0.2\n",
      " -0.04 -0.15 -0.12  0.13 -0.02  0.01  0.06  0.03  0.1   0.08  0.06  0.16\n",
      "  0.02  0.03  0.08  0.08  0.05  0.01  0.1   0.06]\n",
      " Length of the representation 128 and type <type 'numpy.ndarray'>\n",
      "-----\n",
      "\n",
      "Processing faces/face2.jpg.\n",
      "  + Original size: (512, 512, 3)\n",
      "  + Face detection took 0.178661823273 seconds.\n",
      "  + Face alignment took 0.00860714912415 seconds.\n",
      "  + OpenFace forward pass took 0.100596904755 seconds.\n",
      "Representation:\n",
      "[-1.78e-02 -5.93e-02  4.52e-02  3.25e-02 -1.54e-02 -4.77e-02  2.66e-02\n",
      " -4.56e-03 -7.28e-02 -2.68e-02  9.81e-02  9.77e-02  1.01e-01  4.32e-02\n",
      " -1.73e-01  3.77e-02 -1.56e-01  2.45e-02 -1.70e-01  1.67e-01  1.12e-01\n",
      " -2.20e-01  9.30e-02  7.49e-02 -4.33e-02 -3.12e-02  2.66e-03  3.29e-02\n",
      " -1.33e-02  6.17e-03 -1.49e-01 -5.40e-02 -4.87e-03  5.76e-02 -1.73e-01\n",
      " -1.46e-02 -2.17e-01  8.87e-02 -1.42e-01  2.14e-02 -4.23e-02 -8.05e-02\n",
      " -1.21e-02  2.73e-02 -5.41e-02 -1.11e-01  7.97e-03 -2.82e-02 -5.67e-02\n",
      "  1.26e-01  2.06e-02 -6.09e-02 -1.77e-01  5.53e-02  8.70e-02 -1.12e-02\n",
      " -3.24e-02  8.94e-02  9.98e-03  4.63e-02  2.70e-02  2.62e-02  9.01e-02\n",
      "  1.20e-01  4.23e-02  2.38e-02 -1.25e-01 -3.33e-03 -1.00e-01  6.51e-02\n",
      " -1.06e-02  3.70e-02  6.94e-02  7.79e-02  7.29e-02  1.89e-02  1.06e-01\n",
      " -1.66e-02 -2.63e-03 -8.51e-05 -4.21e-02 -2.03e-01  2.18e-02 -2.33e-02\n",
      " -2.11e-01  1.00e-01  5.29e-02  3.41e-02  4.64e-02  1.64e-03  1.46e-03\n",
      " -1.27e-01  9.79e-03 -6.57e-04 -6.54e-02 -1.44e-02 -1.31e-01  3.05e-02\n",
      "  9.53e-02  9.55e-03  2.66e-02  9.72e-03 -1.97e-02  5.55e-02  1.36e-01\n",
      "  8.69e-02 -5.03e-02  1.76e-01  3.29e-02 -2.17e-01 -9.75e-02  1.39e-01\n",
      "  3.89e-02  6.63e-02  1.62e-02  9.13e-02  7.42e-02  1.85e-01  5.42e-02\n",
      "  1.64e-01 -1.23e-01 -4.99e-02 -1.09e-01 -7.77e-03 -6.30e-02  1.54e-01\n",
      " -2.74e-02  5.20e-02]\n",
      " Length of the representation 128 and type <type 'numpy.ndarray'>\n",
      "-----\n",
      "\n",
      "Comparing faces/face1.jpg with faces/face2.jpg.\n",
      "  + Squared l2 distance between representations: 0.644\n"
     ]
    }
   ],
   "source": [
    "img1 = \"faces/face1.jpg\"\n",
    "img2 = \"faces/face2.jpg\"\n",
    "\n",
    "d = getRep(img1) - getRep(img2)\n",
    "print(\"Comparing {} with {}.\".format(img1, img2))\n",
    "print(\"  + Squared l2 distance between representations: {:0.3f}\".format(np.dot(d, d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
