{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different classifiers on data of KdV patients and ID controls\n",
    "Overview of this notebook:\n",
    "\n",
    "First the deepface representations of the cropped images are read in from an Excel file. The data is then plotted by using either t-sne or PCA for dimension reduction. It is clear that there aren't two clear clusters.\n",
    "\n",
    "In the rest of the notebook the following classifiers are tested: k-NN, SVM, Random Forest, Gradient Boosting, AdaBoost, Gaussian Naive Bayes. In the end also an ensemble of all these methods or some of them is tried. None outperforming the Gradient Boosting classifier. \n",
    "\n",
    "To normalize the data either Normalizer (unit form) or StandardScaler (z = (x - mean)/std) is used, without any specific difference in performance yet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dlib_rep(kdv_csv, ID_csv):\n",
    "    data = []\n",
    "    labels = []\n",
    "    no_rep = []\n",
    "    \n",
    "    # get the representations from Excel files kdv\n",
    "    with open (kdv_csv, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for i, row in enumerate(reader):\n",
    "            rep = list(map(float, row[1:]))\n",
    "#             if all(v == 0 for v in rep):\n",
    "#                 no_rep.append(i)\n",
    "#             else:\n",
    "            data.append(rep)\n",
    "            labels.append(1)\n",
    "\n",
    "    # get the representations from Excel files ID control\n",
    "    with open (ID_csv, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for i, row in enumerate(reader):\n",
    "#             if i not in no_rep: \n",
    "#                 rep = list(map(float, row[1:]))\n",
    "            data.append(rep)\n",
    "            labels.append(0)\n",
    "\n",
    "    print(\"All image representations are read in.\")\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_deepface_rep(kdv_csv, ID_csv):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # get the representations from Excel files kdv\n",
    "    with open (kdv_csv, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            rep = list(map(float, row[1:]))\n",
    "            data.append(rep)\n",
    "            labels.append(1)\n",
    "\n",
    "    # get the representations from Excel files ID control\n",
    "    with open (ID_csv, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            rep = list(map(float, row[1:]))\n",
    "            data.append(rep)\n",
    "            labels.append(0)\n",
    "\n",
    "    print(\"All image representations are read in.\")\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_tsne(data, labels, lowest_age = -1, highest_age = -1):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot([1,2])\n",
    "\n",
    "    # visualize data in tnse (men/women)\n",
    "    X_embedded_tsne = TSNE(n_components=2, init='pca').fit_transform(data)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    unique = list(set(labels))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [X_embedded_tsne[j, 0] for j  in range(len(X_embedded_tsne[:,0])) if labels[j] == u]\n",
    "        yi = [X_embedded_tsne[j, 1] for j  in range(len(X_embedded_tsne[:,1])) if labels[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors[i]], label=str(u))\n",
    "    plt.legend()\n",
    "    plt.title(\"t-sne for age range {}-{}\".format(lowest_age, highest_age))\n",
    "\n",
    "    # visualize data in pca (men/women)\n",
    "    X_embedded_pca = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    unique = list(set(labels))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [X_embedded_pca[j, 0] for j  in range(len(X_embedded_pca[:,0])) if labels[j] == u]\n",
    "        yi = [X_embedded_pca[j, 1] for j  in range(len(X_embedded_pca[:,1])) if labels[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors[i]], label=str(u))\n",
    "    plt.legend()\n",
    "    plt.title(\"pca for age range{}-{}\".format(lowest_age, highest_age))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred): \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    plt.figure(1, figsize=(12,6))\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='LOOCV ROC (AUC = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Chance level', alpha=.8)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_loo(model, data, labels):\n",
    "    all_y = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    for train, test in loo.split(data):\n",
    "        all_y.append(labels[test])\n",
    "        model = model.fit(data[train], labels[train])\n",
    "        all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "        all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "        \n",
    "    aroc = roc_auc_score(all_y, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn) \n",
    "    \n",
    "    return aroc, spec, sens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(data, labels):\n",
    "    k_values = [3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    best_aroc = 0\n",
    "    best_k = 0\n",
    "\n",
    "    for k in tqdm(k_values):\n",
    "        for normalize in [0, 1, 2]:\n",
    "            \n",
    "            model = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "            aroc, spec, sens = calculate_metrics_loo(model, data, labels)\n",
    "           \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, normalize \n",
    "                best_k = k\n",
    "                \n",
    "    return best_k, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(data, labels):\n",
    "    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    best_aroc = 0\n",
    "    best_kernel = None\n",
    "\n",
    "    for k in tqdm(kernels):\n",
    "        for normalize in [0, 1, 2]:\n",
    "            \n",
    "            model = SVC(kernel=k, probability=True)\n",
    "            aroc, spec, sens = calculate_metrics_loo(model, data, labels)\n",
    "           \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, normalize \n",
    "                best_kernel = k\n",
    "                \n",
    "    return best_kernel, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    best_estimator_rf = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for normalize in [0, 1, 2]:\n",
    "            \n",
    "            model = RandomForestClassifier(n_estimators=est)\n",
    "            aroc, spec, sens = calculate_metrics_loo(model, data, labels)\n",
    "           \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, normalize \n",
    "                best_estimator_rf = est\n",
    "                \n",
    "    return best_estimator_rf, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20, 40, 60, 80, 100]\n",
    "    best_estimator_gr = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for normalize in [0, 1, 2]:\n",
    "            \n",
    "            model = GradientBoostingClassifier(n_estimators=est, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "            aroc, spec, sens = calculate_metrics_loo(model, data, labels)\n",
    "           \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, normalize \n",
    "                best_estimator_gr = est\n",
    "                \n",
    "    return best_estimator_gr, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20, 40, 60, 80, 100]\n",
    "    best_estimator_ada = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for normalize in [0,1, 2]:\n",
    "\n",
    "            model = AdaBoostClassifier(n_estimators=est, random_state=0)\n",
    "            aroc, spec, sens = calculate_metrics_loo(model, data, labels)\n",
    "           \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, normalize \n",
    "                best_estimator_ada = est\n",
    "                \n",
    "    return best_estimator_ada, best_norm, best_aroc, best_spec, best_sens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 6: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for normalize in tqdm([0,1, 2]):\n",
    "        model = GaussianNB()\n",
    "        aroc, spec, sens = calculate_metrics_loo(model, data, labels)\n",
    "\n",
    "        if aroc > best_aroc:\n",
    "            best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, normalize \n",
    "                \n",
    "    return best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 7: Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_classifiers(k, kernel, n_trees_rf, n_trees_gr, n_trees_ada, data, labels):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    svm = SVC(kernel=kernel, probability=True)\n",
    "    random_forest = RandomForestClassifier(n_estimators=n_trees_rf) \n",
    "    gr_clf = GradientBoostingClassifier(n_estimators=n_trees_gr, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    ada_clf = AdaBoostClassifier(n_estimators=n_trees_ada, random_state=0)\n",
    "    nb = GaussianNB()\n",
    "\n",
    "    classifiers = [('knn', knn), ('svm', svm),('gr_clf', gr_clf), ('random_forest', random_forest), ('nb', nb), ('ada_clf', ada_clf)]\n",
    "    \n",
    "    cv = int(len(labels)/2)\n",
    "\n",
    "    classifier_combinations = []\n",
    "    for L in range(1, len(classifiers)+1):\n",
    "        for subset in itertools.combinations(classifiers, L):\n",
    "            classifier_combinations.append(subset)\n",
    "    \n",
    "    best_ensemble_acc = 0\n",
    "    best_classifier_com = None\n",
    "    best_norm = -1\n",
    "    best_vote = None\n",
    "\n",
    "    for classifier_com in classifier_combinations:\n",
    "    #for classifier_com in tqdm(classifier_combinations):\n",
    "        \n",
    "        for normalize in [0, 1, 2]:\n",
    "            for vote_sort in ['soft', 'hard']:\n",
    "\n",
    "                weights = np.ones(len(classifier_com)).tolist()\n",
    "                ensemble_clf = VotingClassifier(estimators=classifier_com,\n",
    "                                        voting=vote_sort,\n",
    "                                        weights=weights)\n",
    "                \n",
    "                            \n",
    "                mean_acc = cross_val_classifier(ensemble_clf, data, labels, normalize)\n",
    "                \n",
    "                clf_string = \"\"\n",
    "                for clf in classifier_com:\n",
    "                    clf_string  = clf_string + \" \" + clf[0]\n",
    "                    \n",
    "                print(\"Ensemble: {}\".format(clf_string))\n",
    "                print(\"With accuracy: {}\".format(mean_acc))\n",
    "                \n",
    "                if mean_acc > best_ensemble_acc:\n",
    "                    best_ensemble_acc = mean_acc\n",
    "                    best_norm = normalize\n",
    "                    best_vote = vote_sort\n",
    "                    best_classifier_com = classifier_com\n",
    "    \n",
    "    return best_ensemble_acc, best_classifier_com, best_vote, best_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(data_df, labels_df, data_dlib, labels_dlib):\n",
    "    assert data_df.shape[0] == data_dlib.shape[0]\n",
    "    assert labels_df.shape == labels_dlib.shape\n",
    "    \n",
    "    match = [True for i, j in zip(labels_df, labels_dlib) if i == j]\n",
    "    \n",
    "    if False not in match:\n",
    "        print(\"labels are the same\")\n",
    "    else:\n",
    "        print(\"labels are not the same\")\n",
    "\n",
    "        \n",
    "    # zip lists; if shape of dlib is 2210 instead of 11, concatenate them, otherwise skip them\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for index, (df_i, dlib_i) in enumerate(zip(data_df, data_dlib)):\n",
    "        if len(dlib_i) == 2210:\n",
    "            data.append(df_i.tolist()+dlib_i)\n",
    "            labels.append(labels_df[index])\n",
    "            \n",
    "    for i in range(0, 50):\n",
    "        pca = PCA(n_components=i)\n",
    "        components = pca.fit_transform(data)    \n",
    "        if sum(pca.explained_variance_ratio_) > 0.9:\n",
    "            #best_com = i\n",
    "            break\n",
    "        \n",
    "    #pca = PCA(n_components=best_com)\n",
    "    #components = pca.fit_transform(data)            \n",
    "    \n",
    "    print(\"Explained variance with 22 components: {:.3f}%\".format(sum(pca.explained_variance_ratio_)*100))\n",
    "    \n",
    "    return np.array(components), np.array(labels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    \n",
    "    today = date.today()\n",
    "    \n",
    "    #for method in [\"dlib\"]: #, \"deepface\"]:\n",
    "\n",
    "    results_file = open(\"results_dlib_deepface-pca-22_age_groups_\" + str(today)+\".txt\", \"w\")\n",
    "\n",
    "    age_ranges = [[1, 3], [4, 16], [17, 40], [1, 40]] \n",
    "\n",
    "    for [lowest_age, highest_age] in age_ranges:\n",
    "\n",
    "        method = \"dlib\"\n",
    "\n",
    "        kdv_csv = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\representations\\kdv-patients-\"+method+\"-cropped-age-group-\"+str(lowest_age)+\"-\"+str(highest_age)+\".csv\"    \n",
    "        ID_csv  = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\representations\\ID-controls-\"+method+\"-cropped-age-group-\"+str(lowest_age)+\"-\"+str(highest_age)+\".csv\"\n",
    "\n",
    "        data_dlib, labels_dlib = read_dlib_rep(kdv_csv, ID_csv)\n",
    "\n",
    "        method = \"deepface\"\n",
    "\n",
    "        kdv_csv = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\representations\\kdv-patients-\"+method+\"-cropped-age-group-\"+str(lowest_age)+\"-\"+str(highest_age)+\".csv\"    \n",
    "        ID_csv  = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\representations\\ID-controls-\"+method+\"-cropped-age-group-\"+str(lowest_age)+\"-\"+str(highest_age)+\".csv\"\n",
    "\n",
    "        data_df, labels_df = read_deepface_rep(kdv_csv, ID_csv)\n",
    "\n",
    "        # CHECK METHOD\n",
    "        #if method == \"dlib\":\n",
    "        #data_df, labels_df = read_deepface_rep(kdv_csv, ID_csv)\n",
    "        #if method ==\"deepface\":\n",
    "\n",
    "        data, labels = concatenate(data_df, labels_df, data_dlib, labels_dlib)\n",
    "\n",
    "        # plot representation\n",
    "        #plot_pca_tsne(data, labels, lowest_age, highest_age)\n",
    "\n",
    "        # write results and also the parameters to a file \n",
    "        # print(\"Best accuracies to classify the age group \" + str(lowest_age) + \"-\" + str(highest_age))\n",
    "        results_file.write(\"Classifier results for kdv-control age group \" + str(lowest_age) + \"-\" + str(highest_age) + \"\\n\")\n",
    "\n",
    "        # apply classifiers\n",
    "        # print(\"K-NN classifier:\")\n",
    "        k, knn_norm, knn_aroc, knn_spec, knn_sens = knn_classifier(data, labels)\n",
    "        results_file.write(\"knn classifier (k = {}), normalize : {} \\nspecificity: {}, sensitivity: {}, AROC: {} \\n\".format(k, knn_norm, knn_spec, knn_sens, knn_aroc))\n",
    "\n",
    "        # print(\"SVM classifier:\")\n",
    "        kernel, svm_norm, svm_aroc, svm_spec, svm_sens = svm_classifier(data, labels)\n",
    "        results_file.write(\"svm classifier (k = {}), normalize : {} \\nspecificity: {}, sensitivity: {}, AROC: {} \\n\".format(kernel, svm_norm, svm_spec, svm_sens, svm_aroc))\n",
    "\n",
    "        # print(\"Random Forest classifier:\")\n",
    "        n_trees_rf, rf_norm, rf_aroc, rf_spec, rf_sens = rf_classifier(data, labels)\n",
    "        results_file.write(\"Random Forest classifier (trees = {}), normalize : {} \\nspecificity: {}, sensitivity: {}, AROC: {} \\n\".format(n_trees_rf, rf_norm, rf_spec, rf_sens, rf_aroc))\n",
    "\n",
    "        # print(\"Gradient Boost classifier:\")\n",
    "        n_trees_gr, gr_norm, gr_aroc, gr_spec, gr_sens = gr_classifier(data, labels)\n",
    "        results_file.write(\"Gradient Boost classifier (trees = {}), normalize : {} \\nspecificity: {}, sensitivity: {}, AROC: {} \\n\".format(n_trees_gr, gr_norm, gr_spec, gr_sens, gr_aroc))\n",
    "\n",
    "        # print(\"Ada Boost classifier:\")\n",
    "        n_trees_ada, ada_norm, ada_aroc, ada_spec, ada_sens = ada_classifier(data, labels)\n",
    "        results_file.write(\"Ada Boost classifier (trees = {}), normalize : {} \\nspecificity: {}, sensitivity: {}, AROC: {} \\n\".format(n_trees_ada, ada_norm, ada_spec, ada_sens, ada_aroc))\n",
    "\n",
    "#         ensemble_acc, clf_combination, best_vote, ensemble_norm = ensemble_classifiers(k, kernel, n_trees_rf, n_trees_gr, n_trees_ada, data, labels) \n",
    "#         print(\"ensemble classifier has acc: {:.3f} \\n with ensemble: {}\".format(ensemble_acc, str(clf_combination)))\n",
    "#         print(\"\\n \\n \\n\")\n",
    "\n",
    "#         results_file.write(\"ensemble classifier with acc: \" + str(ensemble_acc) + \" with the combination \" + str(clf_combination) + \" and best vote \" + str(best_vote) + \" and normalize = \" + str(ensemble_norm) + \"\\n\")\n",
    "#         results_file.write(\"\\n \\n \")  \n",
    "\n",
    "        results_file.write(\"\\n\")\n",
    "\n",
    "    results_file.close()\n",
    "    print(\"done running main file\")\n",
    "        \n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 7: VotingClassifier with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/rmferg/soft-voting-classifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1, random_state=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "t0 = time.clock()\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=1)\n",
    "svm = SVC(probability=True, kernel='rbf')\n",
    "knn = KNeighborsClassifier(p=2, metric='minkowski')\n",
    "nb = GaussianNB()\n",
    "eclf = VotingClassifier(estimators=[('tree', tree), ('svm', svm), ('knn', knn),('nb', nb)], voting='soft')\n",
    "\n",
    "param_range10 = [.001, .01, 1, 10, 100]\n",
    "param_range1 = list(range(3, 8))\n",
    "param_grid = [{'svm__C':param_range10, 'svm__gamma':param_range10, 'tree__max_depth':param_range1, \n",
    "               'knn__n_neighbors':param_range1}]\n",
    "\n",
    "gs = GridSearchCV(estimator=eclf, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "gs = gs.fit(X_train_std, y_train)\n",
    "\n",
    "print('Best accuracy score: %.3f \\nBest parameters: %s' % (gs.best_score_, gs.best_params_))\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train_std, y_train)\n",
    "t1 = time.clock()\n",
    "print('Running time: %.3f' % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test_std)\n",
    "print('ROC AUC: %.3f \\nAccuracy: %.3f \\nConfusion Matrix:' % (roc_auc_score(y_true=y_test, y_score=y_pred),\n",
    "                                         accuracy_score(y_true=y_test, y_pred=y_pred)))\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
