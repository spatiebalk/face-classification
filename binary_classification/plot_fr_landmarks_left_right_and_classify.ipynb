{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot facereader points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import itertools\n",
    "from scipy.spatial import distance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifiers_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(length):\n",
    "    pairs = []\n",
    "    combs = [comb for comb in itertools.combinations([*range(0, length)], 2)]\n",
    "    for comb in combs:\n",
    "        a = comb[0]\n",
    "        b = comb[1]  \n",
    "        pairs.append([a, b])\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def get_distances(keypoints):\n",
    "    feats  = []\n",
    "    combs = [comb for comb in itertools.combinations([*range(0, len(keypoints))], 2)]\n",
    "    for comb in combs:\n",
    "        a = comb[0]\n",
    "        b = comb[1]\n",
    "        feats.append(distance.euclidean(keypoints[a], keypoints[b]))\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From landmark to all distances for syndromic files - run only once"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\"\n",
    "file = GENERAL_DIR+ \"\\\\features_facereader_landmarks_patient_groups.csv\"\n",
    "\n",
    "syn_rep = []\n",
    "with open(file, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        landmarks = []\n",
    "        i = 1\n",
    "        while i < len(row[1:]):\n",
    "            landmarks.append((float(row[i]), float(row[i+1]), float(row[i+2])))\n",
    "            i+=3    \n",
    "        feats = get_distances(landmarks)\n",
    "        syn_rep.append([row[0]] + feats)\n",
    "        \n",
    "csv_file_syn = GENERAL_DIR+ \"\\\\features_facereader_landmarks_distances_patient_groups.csv\"\n",
    "\n",
    "with open(csv_file_syn, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(syn_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From landmark to all distances for control files - run only once"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\"\n",
    "file = GENERAL_DIR+ \"\\\\features_facereader_landmarks_all_controls.csv\"\n",
    "\n",
    "ID_rep = []\n",
    "with open(file, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        landmarks = []\n",
    "        i = 1\n",
    "        while i < len(row[1:]):\n",
    "            landmarks.append((float(row[i]), float(row[i+1]), float(row[i+2])))\n",
    "            i+=3    \n",
    "        feats = get_distances(landmarks)\n",
    "        ID_rep.append([row[0]] + feats)\n",
    "        \n",
    "csv_file_ID = GENERAL_DIR+ \"\\\\features_facereader_landmarks_distances_all_controls.csv\"\n",
    "\n",
    "with open(csv_file_ID, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(ID_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get most important distance for RF classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features(syn_name, data_dir, nr_feats): \n",
    "    method = \"facereader-landmarks-distances\"\n",
    "    syn_csv = data_dir+\"\\\\representations\\{}-patients-{}.csv\".format(syn_name, method)\n",
    "    ID_csv  = data_dir+\"\\\\representations\\ID-controls-{}.csv\".format(method)\n",
    "    data_fr, labels_fr = classifiers_general.read_rep(syn_name, syn_csv, ID_csv, data_dir)\n",
    "\n",
    "    data, labels  = [], []\n",
    "    for index, data_i in enumerate(data_fr):\n",
    "        if not all(v == 0 for v in data_i):\n",
    "            data.append(data_i) \n",
    "            labels.append(labels_fr[index])\n",
    "\n",
    "    forest = RandomForestClassifier(n_estimators=10,random_state=0) # 10 has been found with best aroc scores\n",
    "    forest.fit(data, labels)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    important_indices = indices[0:nr_feats] \n",
    "\n",
    "    data2 = []\n",
    "    for row in data:\n",
    "        data2.append(np.array(row)[important_indices])                                \n",
    "    data = data2\n",
    "                \n",
    "    return np.array(data), np.array(labels), important_indices\n",
    "\n",
    "def get_keypoints_from_indices(indices):\n",
    "    combs = [comb for comb in itertools.combinations([*range(0, 510)], 2)]\n",
    "    keypoints = []\n",
    "    for index, comb in enumerate(combs):\n",
    "        if index in indices:              \n",
    "            a = comb[0]\n",
    "            b = comb[1]\n",
    "            keypoints.append([a, b])\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_landmarks(patient, GENERAL_DIR):\n",
    "    face_x, face_y, face_z = [], [], []    \n",
    "    file = GENERAL_DIR + \"\\\\features_facereader_landmarks_patient_groups.csv\"\n",
    "\n",
    "    with open(file, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            if patient in row[0]:\n",
    "                print(row[0])\n",
    "                indexed_keypoints = []\n",
    "                i = 1\n",
    "                while i < len(row[1:]):\n",
    "                    indexed_keypoints.append([float(row[i]), float(row[i+1]), float(row[i+2])])\n",
    "                    face_x.append(float(row[i]))\n",
    "                    face_y.append(float(row[i+1]))\n",
    "                    face_z.append(float(row[i+2]))\n",
    "                    i+=3    \n",
    "                break  \n",
    "    return np.array(face_x), np.array(face_y), np.array(face_z), indexed_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_keypoints(important_keypoints, indexed_keypoints):\n",
    "    keypoints3D = []       \n",
    "    for [a, b] in important_keypoints:      \n",
    "        keypoints3D.append([indexed_keypoints[a], indexed_keypoints[b]])      \n",
    "    return keypoints3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split faces (before left/right indices were known)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_face(x, y, z, x_top, x_bot, y_top, y_bot):\n",
    "    middle = [288, 298, 299, 262, 263, 304, 302, 168, 297, 294, 295, 296, 292, 302, 282, 284,\n",
    "                304, 305, 306, 269, 213, 210, 278, 281, 217, 157, 158, 173, 174, 190, 191, 205]\n",
    "    x_r, y_r, z_r, x_l, y_l, z_l = [], [], [], [], [], []\n",
    "    right, left = [], []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        d1 = (x[i]-x_top[0])*(y_top[1]-y_top[0]) - (y[i]-y_top[0])*(x_top[1]-x_top[0]) \n",
    "        d2 = (x[i]-x_bot[0])*(y_bot[1]-y_bot[0]) - (y[i]-y_bot[0])*(x_bot[1]-x_bot[0]) \n",
    "        \n",
    "        if i in middle:\n",
    "            x_r.append(x[i])\n",
    "            y_r.append(y[i])\n",
    "            z_r.append(z[i])\n",
    "            x_l.append(x[i])\n",
    "            y_l.append(y[i])\n",
    "            z_l.append(z[i])\n",
    "            right.append(i)\n",
    "            left.append(i)\n",
    "        else: \n",
    "            if d1 < 0 and d2 < 0 :\n",
    "                x_r.append(x[i])\n",
    "                y_r.append(y[i])\n",
    "                z_r.append(z[i])\n",
    "                right.append(i)\n",
    "            else:\n",
    "                x_l.append(x[i])\n",
    "                y_l.append(y[i])\n",
    "                z_l.append(z[i])\n",
    "                left.append(i)\n",
    "                \n",
    "    return x_l, y_l, z_l, x_r, y_r, z_r, left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = [0, 1, 2, 3, 4, 5, 6, 7, 8, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341]\n",
    "right = [9, 10, 11, 12, 13, 14, 15, 16, 17, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 157, 158, 168, 173, 174, 190, 191, 205, 210, 213, 217, 262, 263, 269, 278, 281, 282, 284, 288, 292, 294, 295, 296, 297, 298, 299, 302, 304, 305, 306, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot\n",
    "from pylab import figure\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "syn_name = 'KDVS'\n",
    "patient = 'KDVS_1.jpg'\n",
    "GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\"\n",
    "x, y, z, indexed_keypoints = load_landmarks(patient, GENERAL_DIR)\n",
    "\n",
    "fig = figure(figsize=(10,10))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    if i in left:\n",
    "        ax.scatter(x[i], y[i], z[i], color='b') \n",
    "    else:\n",
    "        ax.scatter(x[i], y[i], z[i], color='r')\n",
    "    \n",
    "\n",
    "# # Calculate the coefficients. This line answers the initial question. \n",
    "# coeff_top = np.polyfit(x_top, y_top, 1)\n",
    "# coeff_bot = np.polyfit(x_bot, y_bot, 1)\n",
    "\n",
    "# # Let's compute the values of the line...\n",
    "# poly_top = np.poly1d(coeff_top)\n",
    "# poly_bot = np.poly1d(coeff_bot)\n",
    "# x_axis_top = np.linspace(min(x_top), max(x_top),100)\n",
    "# x_axis_bot = np.linspace(min(x_bot), max(x_bot),100)\n",
    "# y_axis_top = poly_top(x_axis_top)\n",
    "# y_axis_bot = poly_bot(x_axis_bot)\n",
    "# z_axis_top = np.ones(100)* np.mean([z[295], z[288]])\n",
    "# z_axis_bot = np.ones(100)* np.mean([z[174], z[295]])\n",
    "\n",
    "# # ...and plot the points and the line\n",
    "# ax.plot(x_axis_top, y_axis_top, z_axis_top)\n",
    "# ax.plot(x_axis_bot, y_axis_bot, z_axis_bot)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save distance data in seperate right and left csv files - patients (only once)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\"\n",
    "file = GENERAL_DIR+ \"\\\\features_facereader_landmarks_patient_groups.csv\"\n",
    "\n",
    "syn_rep_left, syn_rep_right = [],[]\n",
    "with open(file, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        landmarks_left, landmarks_right = [], []\n",
    "        i = 1\n",
    "        count = 0\n",
    "        \n",
    "        while i < len(row[1:]):\n",
    "            if count in left:\n",
    "                landmarks_left.append((float(row[i]), float(row[i+1]), float(row[i+2])))\n",
    "            if count in right:\n",
    "                landmarks_right.append((float(row[i]), float(row[i+1]), float(row[i+2])))       \n",
    "            count += 1\n",
    "            i+=3    \n",
    "        \n",
    "        feats_left = get_distances(landmarks_left)\n",
    "        feats_right = get_distances(landmarks_right)\n",
    "        syn_rep_left.append([row[0]] + feats_left)\n",
    "        syn_rep_right.append([row[0]] + feats_right)\n",
    "            \n",
    "csv_syn_left = GENERAL_DIR+ \"\\\\features_facereader_landmarks_distances_patient_groups_left.csv\"\n",
    "csv_syn_right = GENERAL_DIR+ \"\\\\features_facereader_landmarks_distances_patient_groups_right.csv\"\n",
    "\n",
    "with open(csv_syn_left, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(syn_rep_left)\n",
    "    \n",
    "with open(csv_syn_right, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(syn_rep_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save distance data in seperate right and left csv files - controls (only once)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\"\n",
    "file = GENERAL_DIR+ \"\\\\features_facereader_landmarks_all_controls.csv\"\n",
    "\n",
    "control_rep_left, control_rep_right = [], []\n",
    "with open(file, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        landmarks_left, landmarks_right = [], []\n",
    "        i = 1\n",
    "        count = 0\n",
    "        \n",
    "        while i < len(row[1:]):\n",
    "            if count in left:\n",
    "                landmarks_left.append((float(row[i]), float(row[i+1]), float(row[i+2])))\n",
    "            if count in right:\n",
    "                landmarks_right.append((float(row[i]), float(row[i+1]), float(row[i+2])))\n",
    "            count += 1\n",
    "            i+=3    \n",
    "                \n",
    "        feats_left = get_distances(landmarks_left)\n",
    "        feats_right = get_distances(landmarks_right)\n",
    "        control_rep_left.append([row[0]] + feats_left)\n",
    "        control_rep_right.append([row[0]] + feats_right)\n",
    "        \n",
    "csv_control_left = GENERAL_DIR+ \"\\\\features_facereader_landmarks_distances_all_controls_left.csv\"\n",
    "csv_control_right = GENERAL_DIR+ \"\\\\features_facereader_landmarks_distances_all_controls_right.csv\"\n",
    "\n",
    "with open(csv_control_left, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(control_rep_left)\n",
    "    \n",
    "with open(csv_control_right, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(control_rep_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(syn, side, GENERAL_DIR):\n",
    "    # files with representations\n",
    "    syn_csv = GENERAL_DIR+ \"\\\\features_facereader_landmarks_distances_patient_groups_{}.csv\".format(side)\n",
    "    ID_csv = GENERAL_DIR+ \"\\\\features_facereader_landmarks_distances_all_controls_{}.csv\".format(side)\n",
    "    \n",
    "    # open directories\n",
    "    syn_dir = GENERAL_DIR+\"\\\\{}\\\\{}-patients\".format(syn, syn)\n",
    "    ID_dir = GENERAL_DIR+ \"\\\\{}\\\\{}-selected-ID-controls\".format(syn, syn)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_syn = [f for f in listdir(syn_dir) if (isfile(join(syn_dir, f))) and \".jpg\" in f]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f))) and \".jpg\" in f]\n",
    "    \n",
    "    data, labels, data_syn, data_ID = [], [], [], []\n",
    "    \n",
    "    with open (syn_csv, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for index, row in enumerate(reader):\n",
    "            if row[0] in files_syn: \n",
    "                rep = list(map(float, row[1:]))\n",
    "                data_syn.append(rep)\n",
    "                    \n",
    "    with open (ID_csv, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for index, row in enumerate(reader):\n",
    "            if row[0] in files_ID:\n",
    "                rep = list(map(float, row[1:]))\n",
    "                data_ID.append(rep)\n",
    "    \n",
    "    for index, (syn_item, ID_item) in enumerate(zip(data_syn, data_ID)):\n",
    "        if len(syn_item) == 36315 and len(ID_item) == 36315: # hardcoded for now\n",
    "            data.append(np.array(syn_item))\n",
    "            labels.append(1)\n",
    "            data.append(np.array(ID_item))\n",
    "            labels.append(0)\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot important features for both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [01:37<00:00, 97.43s/it]\n"
     ]
    }
   ],
   "source": [
    "GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\"\n",
    "syn_list = ['ADNP'] #, 'CDK13', 'DEAF1', 'DYRK1A', 'EHMT1', 'FBXO11', 'SON', 'WAC', 'YY1', 'KDVS']\n",
    "\n",
    "# open file for scores \n",
    "results_file = open(\"results_landmarks/results_landmarks_left_right.txt\", \"w\")\n",
    "importance_indices_list = []\n",
    "\n",
    "for syn in tqdm(syn_list): \n",
    "    indices_left, indices_right = [], []\n",
    "\n",
    "    for side in [\"left\", \"right\"]:\n",
    "        data, labels = load_data(syn, side, GENERAL_DIR)\n",
    "        data = Normalizer().fit_transform(data)\n",
    "\n",
    "        results_file.write(\"Syndrome {} with {} patients and {} controls - {}\\n\".format(syn, labels.tolist().count(1), labels.tolist().count(0), side))\n",
    "\n",
    "        ### RANDOM FOREST SCORE ###\n",
    "        data = Normalizer().fit_transform(data)\n",
    "        all_y, all_probs, all_preds = [], [], [] \n",
    "        loo = LeaveOneOut()\n",
    "\n",
    "        for train, test in loo.split(data):\n",
    "            all_y.append(labels[test])\n",
    "            model = RandomForestClassifier(n_estimators=10)\n",
    "            model = model.fit(data[train], labels[train])\n",
    "            all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "            all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "        # based on all predictions make aroc curve and confusion matrix\n",
    "        aroc = roc_auc_score(all_y, all_probs)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "        spec = tn / (tn+fp)  \n",
    "        sens = tp / (tp+fn)\n",
    "        results_file.write(\"AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\\n\".format(aroc, spec, sens))\n",
    "        \n",
    "        ### SAVE MOST IMPORTANT INDICES LEFT/RIGHT ###\n",
    "        forest = RandomForestClassifier(n_estimators=10,random_state=0) # 10 has been found with best aroc scores\n",
    "        forest.fit(data, labels)\n",
    "        importances = forest.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        nr_feat = 10\n",
    "        important_indices = indices[0:nr_feat] # aantal features\n",
    "        print(\"{} features have an importance of {:.4f}\".format(nr_feat, sum(importances[important_indices])))\n",
    "\n",
    "        if side == \"left\":\n",
    "            indices_left = important_indices\n",
    "            importance_left = sum(importances[important_indices])\n",
    "        else:\n",
    "            indices_right = important_indices\n",
    "            importance_right = sum(importances[important_indices])\n",
    "    \n",
    "    importance_indices_list.append({syn: {\"left\": indices_left.tolist(), \"left_importance\": importance_left, \"right\": indices_right.tolist(), \"right_importance\": importance_right}})\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "new_filename = \"results_landmarks/most_important_features.jsonl\"\n",
    "\n",
    "with open(new_filename, 'w') as outfile:\n",
    "    for index, entry in enumerate(importance_indices_list):\n",
    "        json.dump(entry, outfile)\n",
    "        if index == len(importance_indices_list) - 1:\n",
    "            continue           \n",
    "        outfile.write('\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot most important features  - TO DO for all syndromes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file = GENERAL_DIR+ \"\\\\features_facereader_landmarks_patient_groups.csv\"\n",
    "\n",
    "# Find random image of syndrome and retrieve keypoints \n",
    "with open(file, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        if syn in row[0]:\n",
    "            landmarks_left, landmarks_right = [], []\n",
    "            i = 1\n",
    "            count = 0\n",
    "\n",
    "            while i < len(row[1:]):\n",
    "                if count in left:\n",
    "                    landmarks_left.append((float(row[i]), float(row[i+1]), float(row[i+2])))\n",
    "                elif count in right:\n",
    "                    landmarks_right.append((float(row[i]), float(row[i+1]), float(row[i+2])))\n",
    "                else:\n",
    "                    print(\"Error\")\n",
    "\n",
    "                count += 1\n",
    "                i+=3    \n",
    "            break\n",
    "            \n",
    "fig = figure(figsize=(10,10))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# plot the left and right side\n",
    "for [x, y, z] in landmarks_left:\n",
    "    ax.scatter(x, y, z, color='b') \n",
    "for [x, y, z] in landmarks_right:\n",
    "    ax.scatter(x, y, z, color='r') \n",
    "        \n",
    "# get keypoint pairs of indices most important features\n",
    "pairs = get_pairs(len(left))\n",
    "\n",
    "# plot the most important features (pairwise distance)\n",
    "for index, [a,b] in enumerate(pairs):\n",
    "    if index in indices_left:\n",
    "        [x, y, z] = landmarks_left[a]\n",
    "        [x2, y2, z2] = landmarks_left[b]\n",
    "        ax.plot((x, x2), (y, y2), (z, z2))\n",
    "        \n",
    "    if index in indices_right:\n",
    "        [x, y, z] = landmarks_right[a]\n",
    "        [x2, y2, z2] = landmarks_right[b]\n",
    "        ax.plot((x, x2), (y, y2), (z, z2))\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code was for whole face ###\n",
    "# syn_name = 'KDVS'\n",
    "# patient = 'KDVS_1.jpg'\n",
    "# GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\"\n",
    "\n",
    "# x, y, z, indexed_keypoints = fr_plot(patient, GENERAL_DIR)\n",
    "\n",
    "# data_dir = GENERAL_DIR + \"\\\\{}\".format(syn_name) \n",
    "# nr_feats = 10\n",
    "\n",
    "# _, _, indices = get_important_features(syn_name, data_dir)\n",
    "# important_keypoints = get_keypoints_from_indices(indices)\n",
    "# keypoints_3d = get_patient_keypoints(important_keypoints, indexed_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
