{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different classifiers on data of syndromic patients and ID controls\n",
    "Overview of this notebook:\n",
    "\n",
    "First the deepface representations of the cropped images are read in from an Excel file. The data is then plotted by using either t-sne or PCA for dimension reduction. It is clear that there aren't two clear clusters.\n",
    "\n",
    "In the rest of the notebook the following classifiers are tested: k-NN, SVM, Random Forest, Gradient Boosting, AdaBoost, Gaussian Naive Bayes. In the end also an ensemble of all these methods or some of them is tried. None outperforming the Gradient Boosting classifier. \n",
    "\n",
    "To normalize the data either Normalizer (unit form) or StandardScaler (z = (x - mean)/std) is used, without any specific difference in performance yet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rep(syn_name, syn_csv, ID_csv, data_dir):\n",
    "    \n",
    "    # open directories\n",
    "    syn_dir = data_dir+\"\\\\{}-patients\".format(syn_name)\n",
    "    ID_dir = data_dir+ \"\\\\{}-selected-ID-controls\".format(syn_name)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_syn = [f for f in listdir(syn_dir) if (isfile(join(syn_dir, f))) and \".jpg\" in f]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f))) and \".jpg\" in f]\n",
    "    \n",
    "    data, labels, indices_to_drop = [], [], []\n",
    "\n",
    "    data_syn = []\n",
    "    with open (syn_csv, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for index, row in enumerate(reader):\n",
    "            if row[0] in files_syn: \n",
    "                rep = list(map(float, row[1:]))\n",
    "                data_syn.append(rep)\n",
    "                if all(v == 0 for v in rep):\n",
    "                    indices_to_drop.append(index)\n",
    "                    \n",
    "    data_ID = []                    \n",
    "    with open (ID_csv, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for index, row in enumerate(reader):\n",
    "            if row[0] in files_ID:\n",
    "                rep = list(map(float, row[1:]))\n",
    "                data_ID.append(rep)\n",
    "                if all(v == 0 for v in rep):\n",
    "                    indices_to_drop.append(index)\n",
    "    \n",
    "\n",
    "    for index, (syn_item, ID_item) in enumerate(zip(data_syn, data_ID)):\n",
    "        if index not in indices_to_drop:\n",
    "            data.append(syn_item)\n",
    "            labels.append(1)\n",
    "            data.append(ID_item)\n",
    "            labels.append(0)\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rep2(syn_name, syn_csv, ID_csv, data_dir):\n",
    "    \n",
    "    # open directories\n",
    "    syn_dir = data_dir+\"\\\\{}-patients\".format(syn_name)\n",
    "    ID_dir = data_dir+ \"\\\\{}-selected-ID-controls\".format(syn_name)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_syn = [f for f in listdir(syn_dir) if (isfile(join(syn_dir, f))) and \".jpg\" in f]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f))) and (\".jpg\" in f or \".JPG\" in f) ]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, csv_file in enumerate([ID_csv, syn_csv]):\n",
    "        with open (csv_file, newline='') as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if row[0] in files_syn or row[0] + \".jpg\" in files_ID or row[0] + \".JPG\" in files_ID:\n",
    "                    rep = list(map(float, row[1:]))\n",
    "                    data.append(row)\n",
    "                    labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_tsne(data, labels, lowest_age = -1, highest_age = -1):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot([1,2])\n",
    "\n",
    "    # visualize data in tnse (men/women)\n",
    "    X_embedded_tsne = TSNE(n_components=2, init='pca').fit_transform(data)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    unique = list(set(labels))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [X_embedded_tsne[j, 0] for j  in range(len(X_embedded_tsne[:,0])) if labels[j] == u]\n",
    "        yi = [X_embedded_tsne[j, 1] for j  in range(len(X_embedded_tsne[:,1])) if labels[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors[i]], label=str(u))\n",
    "    plt.legend()\n",
    "    plt.title(\"t-sne for age range {}-{}\".format(lowest_age, highest_age))\n",
    "\n",
    "    # visualize data in pca (men/women)\n",
    "    X_embedded_pca = PCA(n_components=2).fit_transform(data)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    unique = list(set(labels))\n",
    "    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [X_embedded_pca[j, 0] for j  in range(len(X_embedded_pca[:,0])) if labels[j] == u]\n",
    "        yi = [X_embedded_pca[j, 1] for j  in range(len(X_embedded_pca[:,1])) if labels[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors[i]], label=str(u))\n",
    "    plt.legend()\n",
    "    plt.title(\"pca for age range{}-{}\".format(lowest_age, highest_age))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred): \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    plt.figure(1, figsize=(12,6))\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='LOOCV ROC (AUC = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Chance level', alpha=.8)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, i):\n",
    "\n",
    "    if i == 0:\n",
    "        return data\n",
    "    \n",
    "    if i == 1:\n",
    "        return Normalizer().fit_transform(data)\n",
    "        \n",
    "    if i == 2:\n",
    "        return StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = list(range(4))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(data, labels):\n",
    "    k_values = [3,5,7,9,11,13,15]\n",
    "    best_aroc = 0\n",
    "    best_k = 0\n",
    "    best_aroc, best_spec, best_sens = -1, -1, -1\n",
    "\n",
    "#     for k in tqdm(k_values):\n",
    "#         # can't have more neighbors than samples\n",
    "#         if k < data.shape[0]:\n",
    "#             for i in [0, 1, 2]:\n",
    "    data = normalize(data, 1) \n",
    "    all_y, all_probs, all_preds = [], [], [] \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # leave one out split and make prediction\n",
    "    for train, test in loo.split(data):\n",
    "        all_y.append(labels[test])\n",
    "        model = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "        model = model.fit(data[train], labels[train])                \n",
    "        all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "        all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "    # based on all predictions make aroc curve and confusion matrix\n",
    "    aroc = roc_auc_score(all_y, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn)\n",
    "\n",
    "    if aroc > best_aroc:\n",
    "        best_aroc, best_spec, best_sens = aroc, spec, sens\n",
    "\n",
    "    return 3, 1, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(data, labels):\n",
    "    #kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    best_aroc, best_spec, best_sens = -1, -1, -1\n",
    "    #best_kernel = None\n",
    "\n",
    "#     for k in tqdm(kernels):\n",
    "#         for i in [0, 1, 2]:\n",
    "            \n",
    "    data = normalize(data, 1) \n",
    "    all_y, all_probs, all_preds = [], [], [] \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # leave one out split and make prediction\n",
    "    for train, test in loo.split(data):\n",
    "        all_y.append(labels[test])\n",
    "        model = SVC(kernel=\"rbf\", probability=True)\n",
    "        model = model.fit(data[train], labels[train])\n",
    "        all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "        all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "    # based on all predictions make aroc curve and confusion matrix\n",
    "    aroc = roc_auc_score(all_y, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn)\n",
    "\n",
    "    if aroc > best_aroc:\n",
    "        best_aroc, best_spec, best_sens = aroc, spec, sens\n",
    "                \n",
    "    return \"rbf\", 1, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20] #, 40, 60, 80]\n",
    "    best_estimator_rf = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for i in [0, 1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = RandomForestClassifier(n_estimators=est)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_estimator_rf = est\n",
    "    \n",
    "    return best_estimator_rf, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    #estimators = [10]#[5, 10, 20] #, 40, 60, 80]\n",
    "    best_estimator_gr = 0\n",
    "    best_norm, best_spec, best_sens = -1, -1, -1\n",
    "\n",
    "    #for est in tqdm(estimators):\n",
    "        \n",
    "        #for i in [1]:# [0, 1, 2]:\n",
    "    est = 10        \n",
    "    data = normalize(data, 1) \n",
    "    all_y, all_probs, all_preds = [], [], [] \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # leave one out split and make prediction\n",
    "    for train, test in loo.split(data):\n",
    "        all_y.append(labels[test])\n",
    "        model = GradientBoostingClassifier(n_estimators=est)\n",
    "        model = model.fit(data[train], labels[train])\n",
    "        all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "        all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "    # based on all predictions make aroc curve and confusion matrix\n",
    "    aroc = roc_auc_score(all_y, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn)\n",
    "\n",
    "    if aroc > best_aroc:\n",
    "        best_aroc, best_spec, best_sens = aroc, spec, sens\n",
    "        #best_estimator_gr = est\n",
    "                \n",
    "#             if best_aroc > 0.8:\n",
    "#                 print(\"tn {}, fp {}, fn {}, tp {}\".format(tn, fp, fn, tp))\n",
    "#                 print(\"aroc: {} , spec: {}, sens: {}\".format(best_aroc, spec, sens))\n",
    "#                 print(\"trees: {}, norm: {}\".format(best_estimator_gr, best_norm))\n",
    "#                 conf_matrix = [[tp, fp],\n",
    "#                              [fn, tn]]\n",
    "#                 df_cm = pd.DataFrame(conf_matrix, index = [\"Syn_pred\", \"Control_pred\"], columns = [\"Syn\", \"Control\"])\n",
    "#                 plt.figure(figsize = (6, 6))\n",
    "#                 sns_heat = sns.heatmap(df_cm, annot=True)\n",
    "#                 plt.show()                \n",
    "                \n",
    "                                \n",
    "    return est, 1, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_classifier(data, labels):\n",
    "    best_aroc = 0\n",
    "    estimators = [5, 10, 20 ] #, 40, 60, 80]\n",
    "    best_estimator_ada = 0\n",
    "    best_norm = -1\n",
    "\n",
    "    for est in tqdm(estimators):\n",
    "        for i in [0,1, 2]:\n",
    "            \n",
    "            data = normalize(data, i) \n",
    "            all_y, all_probs, all_preds = [], [], [] \n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            # leave one out split and make prediction\n",
    "            for train, test in loo.split(data):\n",
    "                all_y.append(labels[test])\n",
    "                model = AdaBoostClassifier(n_estimators=est)\n",
    "                model = model.fit(data[train], labels[train])\n",
    "                all_probs.append(model.predict_proba(data[test].reshape(1, -1))[:,1])\n",
    "                all_preds.append(model.predict(data[test].reshape(1, -1)))\n",
    "\n",
    "            # based on all predictions make aroc curve and confusion matrix\n",
    "            aroc = roc_auc_score(all_y, all_probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_y, all_preds).ravel()\n",
    "            spec = tn / (tn+fp)  \n",
    "            sens = tp / (tp+fn)\n",
    "               \n",
    "            if aroc > best_aroc:\n",
    "                best_aroc, best_spec, best_sens, best_norm = aroc, spec, sens, i \n",
    "                best_estimator_ada = est\n",
    "                \n",
    "    return best_estimator_ada, best_norm, best_aroc, best_spec, best_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(syn_name, data_dir, data_combination, nr_feats): \n",
    "\n",
    "    method = \"deepface\"\n",
    "    syn_csv = data_dir+\"\\\\representations\\{}-patients-{}.csv\".format(syn_name, method)\n",
    "    ID_csv  = data_dir+\"\\\\representations\\ID-controls-{}.csv\".format(method)\n",
    "    data_df, labels_df = read_rep(syn_name, syn_csv, ID_csv, data_dir)\n",
    "    \n",
    "    method = \"dlib\"\n",
    "    syn_csv = data_dir+\"\\\\representations\\{}-patients-{}.csv\".format(syn_name, method)\n",
    "    ID_csv  = data_dir+\"\\\\representations\\ID-controls-{}.csv\".format(method)\n",
    "    data_dlib, labels_dlib = read_rep(syn_name, syn_csv, ID_csv, data_dir)\n",
    "\n",
    "    \n",
    "    if data_combination == 0: # or data_combination == 2 or data_combination == 3:\n",
    "        # only deepface\n",
    "        data = data_df\n",
    "        labels = labels_df\n",
    "    \n",
    "    if data_combination == 1: # or data_combination == 2:\n",
    "        # only dlib\n",
    "        data, labels  = [], []\n",
    "        for index, dlib_i in enumerate(data_dlib):\n",
    "            if not all(v == 0 for v in dlib_i):\n",
    "                #only if a face is found\n",
    "                data.append(dlib_i) # concatenation of 4096 deepface + 2210 dlib\n",
    "                labels.append(labels_dlib[index])\n",
    "                \n",
    "                \n",
    "    if data_combination == 2 or data_combination == 3 or data_combination == 4:# or data_combination == 3 or data_combination == 4:\n",
    "        # deepface + dlib (all features) \n",
    "        data, labels  = [], []\n",
    "        for index, (df_i, dlib_i) in enumerate(zip(data_df, data_dlib)):\n",
    "            if not all(v == 0 for v in dlib_i):\n",
    "                #only if a face is found \n",
    "                if not isinstance(df_i, list):\n",
    "                    df_i = df_i.tolist()\n",
    "                if not isinstance(dlib_i, list):\n",
    "                    dlib_i = dlib_i.tolist()  \n",
    "                    \n",
    "                data.append(df_i+dlib_i) # concatenation of 4096 deepface + 2210 dlib\n",
    "                labels.append(labels_df[index])\n",
    "                \n",
    "                                               \n",
    "    if data_combination == 3:\n",
    "        # deepface + dlib (x most important features)\n",
    "        # data, labels are already filled from the above if statement\n",
    "                                               \n",
    "        # using a Random Forest the x most important features are used                                   \n",
    "        forest = RandomForestClassifier(n_estimators=10,random_state=0) # 10 has been found with best aroc scores\n",
    "        forest.fit(data, labels)\n",
    "        importances = forest.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        indices = indices[0:nr_feats] \n",
    "\n",
    "        data2 = []\n",
    "        for row in data:\n",
    "            data2.append(np.array(row)[indices])                                \n",
    "        data = data2\n",
    "\n",
    "                                               \n",
    "    nr_comps = 0\n",
    "    if data_combination == 4:\n",
    "        # pca components that explain > 0.9 variance\n",
    "        for i in range(0, np.array(data).shape[0]):\n",
    "            pca = PCA(n_components=i)\n",
    "            components = pca.fit_transform(data)    \n",
    "            if sum(pca.explained_variance_ratio_) > 0.9:\n",
    "                nr_comps = i\n",
    "        \n",
    "        pca = PCA(n_components=nr_comps)\n",
    "        data = pca.fit_transform(data)       \n",
    "        \n",
    "    \n",
    "    if data_combination == 5 or data_combination == 7:\n",
    "        # openface \n",
    "        method = \"openface\"\n",
    "        syn_csv = data_dir+\"\\\\representations\\{}-patients-{}.csv\".format(syn_name, method)\n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-{}.csv\".format(method)\n",
    "        data_openface, labels_openface = read_rep2(syn_name, syn_csv, ID_csv, data_dir)\n",
    "        \n",
    "        data = []\n",
    "        openface_names = data_openface[:,0]\n",
    "        data_openface = np.array(data_openface)[:, 1:]\n",
    "        for openface_i in data_openface:\n",
    "            rep = [float(i) for i in openface_i.tolist()]\n",
    "            data.append(rep)\n",
    "\n",
    "        labels = np.array(labels_openface)\n",
    "        \n",
    "        \n",
    "    if data_combination == 6 or data_combination == 7:\n",
    "        # cfps        \n",
    "        method = \"cfps\"\n",
    "        syn_csv = data_dir+\"\\\\representations\\{}-patients-{}.csv\".format(syn_name, method)\n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-{}.csv\".format(method)\n",
    "        data_cfps, labels_cfps = read_rep2(syn_name, syn_csv, ID_csv, data_dir)\n",
    "        \n",
    "        data = []\n",
    "        cfps_names = data_cfps[:,0]\n",
    "        data_cfps = np.array(data_cfps)[:, 1:]\n",
    "        \n",
    "        for cfps_i in data_cfps:\n",
    "            rep = [float(i) for i in cfps_i.tolist()]\n",
    "            data.append(rep)\n",
    "            \n",
    "        labels = np.array(labels_cfps)\n",
    "\n",
    "        \n",
    "    if data_combination == 7:\n",
    "        # openface + cfps \n",
    "           \n",
    "        matches = [i==j for i, j in zip(openface_names, cfps_names)]\n",
    "        \n",
    "        data, labels  = [], []\n",
    "        for index, (openface_i, cfps_i) in enumerate(zip(data_openface, data_cfps)):\n",
    "            if(matches[index]):\n",
    "                if not isinstance(openface_i, list):\n",
    "                    openface_i = openface_i.tolist()\n",
    "                if not isinstance(cfps_i, list):\n",
    "                    cfps_i = cfps_i.tolist()  \n",
    "                    \n",
    "                rep_list = openface_i+cfps_i\n",
    "                rep = [float(i) for i in rep_list]\n",
    "                data.append(rep) # concatenation of 128 openface + 340 cfps\n",
    "                labels.append(labels_openface[index].astype(np.float64))\n",
    "     \n",
    "            \n",
    "    \n",
    "    if data_combination == 8:\n",
    "        # facereader\n",
    "        method = \"facereader\"\n",
    "        syn_csv = data_dir+\"\\\\representations\\{}-patients-{}.csv\".format(syn_name, method)  \n",
    "        ID_csv  = data_dir+\"\\\\representations\\ID-controls-{}.csv\".format(method)\n",
    "\n",
    "        data_fr, labels_fr = read_rep(syn_name, syn_csv, ID_csv, data_dir)      \n",
    "        \n",
    "        data, labels  = [], []\n",
    "        for index, fr_i in enumerate(data_fr):\n",
    "            if not all(v == 0 for v in fr_i):\n",
    "                data.append(fr_i)\n",
    "                labels.append(labels_fr[index])\n",
    "    \n",
    "    return 0, np.array(data), np.array(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header(data_combination, nr_feats):\n",
    "    if data_combination == 0:\n",
    "        return \"0: Classifying data with deepface representation\\n\\n\"\n",
    "        \n",
    "    if data_combination == 1:\n",
    "        return\"1: Classifying data with dlib representation\\n\\n\"\n",
    "            \n",
    "    if data_combination == 2:\n",
    "        return \"2: Classifying data with all deepface+dlib representations\\n\\n\"\n",
    "            \n",
    "    if data_combination == 3:\n",
    "        return \"3: Classifying data with the {} most important features of deepface-dlib representations\\n\\n\".format(nr_feats)\n",
    "        \n",
    "    if data_combination == 4:\n",
    "        return \"4: Classifying data with PCA components of deepface-dlib representation\\n\"\n",
    "    \n",
    "    if data_combination == 5:\n",
    "        return \"5: Classifying data with openface representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 6:\n",
    "        return \"6: Classifying data with cfps representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 7:\n",
    "        return \"7: Classifying data with openface+cfps representation\\n\\n\"\n",
    "    \n",
    "    if data_combination == 8:\n",
    "        return \"8: Classifying data with facereader representation\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syndrome that will be classified: ADNP \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (66, 4096) and labels shape: (66,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (12, 2277) and labels shape: (12,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (12, 6373) and labels shape: (12,)\n",
      "Syndrome that will be classified: ANKRD11 \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (50, 4096) and labels shape: (50,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (20, 2277) and labels shape: (20,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (20, 6373) and labels shape: (20,)\n",
      "Syndrome that will be classified: CDK13 \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (60, 4096) and labels shape: (60,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (26, 2277) and labels shape: (26,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (26, 6373) and labels shape: (26,)\n",
      "Syndrome that will be classified: DEAF1 \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (38, 4096) and labels shape: (38,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (16, 2277) and labels shape: (16,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (16, 6373) and labels shape: (16,)\n",
      "Syndrome that will be classified: DYRK1A \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (32, 4096) and labels shape: (32,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (8, 2277) and labels shape: (8,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (8, 6373) and labels shape: (8,)\n",
      "Syndrome that will be classified: EHMT1 \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (78, 4096) and labels shape: (78,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (32, 2277) and labels shape: (32,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (32, 6373) and labels shape: (32,)\n",
      "Syndrome that will be classified: FBXO11 \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (34, 4096) and labels shape: (34,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (10, 2277) and labels shape: (10,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (10, 6373) and labels shape: (10,)\n",
      "Syndrome that will be classified: KDVS \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (150, 4096) and labels shape: (150,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (62, 2277) and labels shape: (62,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (62, 6373) and labels shape: (62,)\n",
      "Syndrome that will be classified: SON \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (36, 4096) and labels shape: (36,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (10, 2277) and labels shape: (10,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (10, 6373) and labels shape: (10,)\n",
      "Syndrome that will be classified: WAC \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (24, 4096) and labels shape: (24,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (6, 2277) and labels shape: (6,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (6, 6373) and labels shape: (6,)\n",
      "Syndrome that will be classified: YY1 \n",
      "\n",
      "\n",
      "0: Classifying data with deepface representation\n",
      "\n",
      "\n",
      "Data shape: (20, 4096) and labels shape: (20,)\n",
      "1: Classifying data with dlib representation\n",
      "\n",
      "\n",
      "Data shape: (10, 2277) and labels shape: (10,)\n",
      "2: Classifying data with all deepface+dlib representations\n",
      "\n",
      "\n",
      "Data shape: (10, 6373) and labels shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "def main():    \n",
    "        \n",
    "    today = date.today()\n",
    "    GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\" \n",
    "\n",
    "    syn_list = ['ADNP', 'ANKRD11', 'CDK13', 'DEAF1', 'DYRK1A', 'EHMT1', 'FBXO11', 'KDVS', 'SON', 'WAC', 'YY1'] \n",
    "    results_file = open(\"results/all_syndromes_deepface_dlib_knn{}.txt\".format(today), \"w\")\n",
    "    start = time.time()\n",
    "\n",
    "    for syn_name in syn_list:      \n",
    "\n",
    "        data_dir = GENERAL_DIR + \"\\\\{}\".format(syn_name) \n",
    "        results_file.write(\"Syndrome that will be classified: {} \\n\\n\".format(syn_name))\n",
    "        print(\"Syndrome that will be classified: {} \\n\\n\".format(syn_name))\n",
    "\n",
    "        for data_combination in [0,1, 2]: #, 3, 4, 5, 6, 7, 8]: \n",
    "\n",
    "            results_file.write(get_header(data_combination, 0))\n",
    "            print(get_header(data_combination, 0))            \n",
    "\n",
    "            nr_comps, data, labels = concatenate(syn_name, data_dir, data_combination, 0) \n",
    "            \n",
    "            if labels.tolist().count(1) <= 2:\n",
    "                results_file.write(\"NO RESULTS as there are {} patients and {} controls with a representation\\n\\n\".format(labels.tolist().count(1), labels.tolist().count(0)))\n",
    "                continue\n",
    "            \n",
    "            print(\"Data shape: {} and labels shape: {}\".format(data.shape, labels.shape))\n",
    "            \n",
    "            results_file.write(\"Shape of data: {} patients, {} controls, {} features \\n\\n\".format(labels.tolist().count(1), labels.tolist().count(0), data.shape[1]))                          \n",
    "            results_file.write(\"CLASSIFIER RESULTS for {} patients and controls \\n\".format(syn_name))\n",
    "\n",
    "            n_trees_gr, gr_norm, gr_aroc, gr_spec, gr_sens = gr_classifier(data, labels)\n",
    "            results_file.write(\"Gradient Boost classifier (trees = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(n_trees_gr, gr_norm, gr_aroc, gr_spec, gr_sens))\n",
    "           \n",
    "            kernel, svm_norm, svm_aroc, svm_spec, svm_sens = svm_classifier(data, labels)\n",
    "            results_file.write(\"SVM (kernel = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(kernel, svm_norm, svm_aroc, svm_spec, svm_sens))\n",
    "\n",
    "            k, knn_norm, knn_aroc, knn_spec, knn_sens = knn_classifier(data, labels)\n",
    "            results_file.write(\"KNN (k = {}), normalize : {} \\n    AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\".format(k, knn_norm, knn_aroc, knn_spec, knn_sens))\n",
    "     \n",
    "            results_file.write(\"\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    results_file.write(\"Running this file took {:.2f} hours\".format((end-start)/3600.00))\n",
    "    results_file.close()\n",
    "            \n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
