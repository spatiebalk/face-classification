{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load weights of DeepFace\n",
    "2. Add classification layer (2)\n",
    "3. For each syn in syn_list calculate the stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from os import path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image \n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import keras.initializers\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (152, 152) # set by the model \n",
    "CHANNELS = 3 # RGB image\n",
    "NUM_CLASSES = 8631 # classification layer will be removed \n",
    "LEARN_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "DOWNLOAD_PATH = 'https://github.com/swghosh/DeepFace/releases/download/weights-vggface2-2d-aligned/VGGFace2_DeepFace_weights_val-0.9034.h5.zip'\n",
    "MD5_HASH = '0b21fb70cd6901c96c19ac14c9ea8b89'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_init = keras.initializers.RandomNormal(mean=0, stddev=0.01)\n",
    "bias_init = keras.initializers.Constant(value=0.5)\n",
    "\n",
    "def conv2d_layer(**args):\n",
    "    return keras.layers.Conv2D(**args, \n",
    "        kernel_initializer=wt_init, \n",
    "        bias_initializer=bias_init,\n",
    "        activation=keras.activations.relu)\n",
    "def lc2d_layer(**args):\n",
    "    return keras.layers.LocallyConnected2D(**args, \n",
    "        kernel_initializer=wt_init, \n",
    "        bias_initializer=bias_init,\n",
    "        activation=keras.activations.relu)\n",
    "def dense_layer(**args):\n",
    "    return keras.layers.Dense(**args, \n",
    "        kernel_initializer=wt_init, \n",
    "        bias_initializer=bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifying_deepface(image_size=IMAGE_SIZE, channels=CHANNELS, num_classes=NUM_CLASSES, learn_rate=LEARN_RATE, momentum=MOMENTUM):\n",
    "    deepface = keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(*image_size, channels), name='I0'),\n",
    "        conv2d_layer(filters=32, kernel_size=11, name='C1'),\n",
    "        keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same',  name='M2'),\n",
    "        conv2d_layer(filters=16, kernel_size=9, name='C3'),\n",
    "        lc2d_layer(filters=16, kernel_size=9, name='L4'),\n",
    "        lc2d_layer(filters=16, kernel_size=7, strides=2, name='L5'),\n",
    "        lc2d_layer(filters=16, kernel_size=5, name='L6'),\n",
    "        keras.layers.Flatten(name='F0'),\n",
    "        dense_layer(units=4096, activation=keras.activations.relu, name='F7'),\n",
    "        keras.layers.Dropout(rate=0.5, name='D0'),\n",
    "        dense_layer(units=num_classes, activation=keras.activations.softmax, name='F8')\n",
    "    ], name='DeepFace')\n",
    "    # deepface.summary()\n",
    "\n",
    "    sgd_opt = keras.optimizers.SGD(lr=learn_rate, momentum=momentum)\n",
    "    cce_loss = keras.losses.categorical_crossentropy\n",
    "\n",
    "    deepface.compile(optimizer=sgd_opt, loss=cce_loss, metrics=['accuracy'])\n",
    "    \n",
    "    return deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights():\n",
    "    filename = 'deepface.zip'\n",
    "    downloaded_file_path = keras.utils.get_file(filename, DOWNLOAD_PATH, \n",
    "        md5_hash=MD5_HASH, extract=True)\n",
    "    downloaded_h5_file = path.join(path.dirname(downloaded_file_path), \n",
    "        path.basename(DOWNLOAD_PATH).rstrip('.zip'))\n",
    "    return downloaded_h5_file\n",
    "\n",
    "\n",
    "def create_deepface():\n",
    "    model = create_classifying_deepface()\n",
    "    weights = get_weights()\n",
    "    model.load_weights(weights)\n",
    "    \n",
    "    num_classes = 2\n",
    "    x = model.layers[-2].output\n",
    "    x = dense_layer(units=num_classes, activation=keras.activations.softmax, name='preds')(x)\n",
    "    #x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model2 = Model(model.input, x)\n",
    "    \n",
    "    sgd_opt = keras.optimizers.SGD(lr=LEARN_RATE, momentum=MOMENTUM)\n",
    "    cce_loss = keras.losses.categorical_crossentropy\n",
    "    #model2.compile(optimizer=sgd_opt, loss=cce_loss, metrics=['accuracy'])\n",
    " \n",
    "\n",
    "    model2.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "        metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    #model2.summary()\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(syn, GENERAL_DIR):\n",
    "    \n",
    "    data, labels = [], [] \n",
    "    \n",
    "    syn_dir = GENERAL_DIR + \"\\\\{}\\{}-patients\".format(syn, syn)\n",
    "    ID_dir = GENERAL_DIR + \"\\\\{}\\{}-selected-ID-controls\".format(syn, syn)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_syn = [f for f in listdir(syn_dir) if (isfile(join(syn_dir, f)))and \".jpg\" in f]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f))) and \".jpg\" in f]\n",
    "    \n",
    "    print(\"Syn_list: {}, ID_list: {}\".format(len(files_syn), len(files_ID)))\n",
    "\n",
    "    for filename in files_syn:\n",
    "        im = Image.open(join(syn_dir, filename))\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        data.append(np.array(im))\n",
    "        labels.append(1)\n",
    "\n",
    "    for filename in files_ID:\n",
    "        im = Image.open(join(ID_dir, filename))\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        data.append(np.array(im))\n",
    "        labels.append(0)    \n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 152, 152, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-accdf4b74d41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mconv_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheatmap_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#np.argmax(predictions[0])]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \"\"\"\n\u001b[1;32m--> 814\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Use ref() instead.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5321\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5322\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5323\u001b[1;33m       raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n\u001b[0m\u001b[0;32m   5324\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5325\u001b[0m                        \"graph.\")\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "model  = create_deepface()\n",
    "\n",
    "im = Image.open(r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\test\\YY1_1.jpg\")\n",
    "im = np.array(im.resize(IMAGE_SIZE))\n",
    "\n",
    "#im = preprocessing.image.img_to_array(im)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = tf.convert_to_tensor(im, dtype=np.float32)\n",
    "print(im.shape)\n",
    "\n",
    "conv_layer = model.get_layer(\"L6\")\n",
    "heatmap_model = Model([model.input], [conv_layer.output, model.output])\n",
    "\n",
    "with tf.GradientTape() as gtape:\n",
    "    conv_output, predictions = heatmap_model(im)\n",
    "    \n",
    "    i = tf.math.argmax(predictions[0], -1).eval(session=tf.compat.v1.Session()) #.numpy()\n",
    "    \n",
    "    loss = predictions[:, i] #np.argmax(predictions[0])]\n",
    "    grads = gtape.gradient(loss, conv_output)\n",
    "    pooled_grads = K.mean(grads, axis=(0,1,2))\n",
    "\n",
    "heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "max_heat = np.max(heatmap)\n",
    "if max_heat == 0:\n",
    "    max_heat = 1e-10\n",
    "heatmap /= max_heat\n",
    "\n",
    "print(heatmap.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syn_list: 10, ID_list: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 255s 13s/step - loss: 162.6499 - sparse_categorical_accuracy: 0.3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [05:52, 352.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 313s 16s/step - loss: 133.5634 - sparse_categorical_accuracy: 0.3158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [13:12, 378.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 292s 15s/step - loss: 46.3643 - sparse_categorical_accuracy: 0.5263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [20:37, 398.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 313s 16s/step - loss: 117.8461 - sparse_categorical_accuracy: 0.4211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [28:52, 427.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 313s 16s/step - loss: 90.1075 - sparse_categorical_accuracy: 0.4211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [37:15, 450.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 350s 18s/step - loss: 107.6215 - sparse_categorical_accuracy: 0.4737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [46:22, 479.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 313s 16s/step - loss: 180.3759 - sparse_categorical_accuracy: 0.4737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [54:58, 490.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 339s 18s/step - loss: 116.7136 - sparse_categorical_accuracy: 0.5263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [1:04:00, 505.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 322s 17s/step - loss: 166.8113 - sparse_categorical_accuracy: 0.4211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [1:13:07, 518.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [1:17:54, 519.38s/it]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[19,63,63,16] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node gradients_9/L4_9/strided_slice_1831_grad/StridedSliceGrad (defined at C:\\MyPrograms\\Anaconda3\\envs\\ai-thesis2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_1471148]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9d2f124a4cf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_deepface\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0my_pred_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyPrograms\\Anaconda3\\envs\\ai-thesis2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\MyPrograms\\Anaconda3\\envs\\ai-thesis2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[19,63,63,16] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node gradients_9/L4_9/strided_slice_1831_grad/StridedSliceGrad (defined at C:\\MyPrograms\\Anaconda3\\envs\\ai-thesis2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_1471148]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\" \n",
    "\n",
    "# load img data\n",
    "syn_list = ['ADNP', 'ANKRD11', 'CDK13', 'DEAF1', 'DYRK1A', 'EHMT1', 'FBXO11', 'SON', 'WAC', 'YY1', 'KDVS']\n",
    "results_file = open(\"results/deepface_classification.txt\", \"w\")\n",
    "\n",
    "for syn in ['YY1']:\n",
    "    data, labels = load_data(syn, GENERAL_DIR)\n",
    "\n",
    "    results_file.write(\"Syndrome {} with {} patients and {} controls\\n\".format(syn, labels.tolist().count(1), labels.tolist().count(0)))\n",
    "    all_y, all_probs, all_preds = [], [], [] \n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    for train_index, test_index in tqdm(loo.split(data)):\n",
    "        X_train, X_test = np.array(data[train_index]), data[test_index]\n",
    "        y_train, y_test = np.array(labels[train_index]), labels[test_index]\n",
    "\n",
    "        model = create_deepface()\n",
    "        model.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE, epochs=1, shuffle=True)\n",
    "\n",
    "        y_pred_array = model.predict(X_test)\n",
    "        y_pred = tf.math.argmax(y_pred_array, -1).numpy()\n",
    "\n",
    "        all_y.append(y_test[0])\n",
    "        all_probs.append(y_pred_array[0][1])\n",
    "        all_preds.append(y_pred)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    aroc = roc_auc_score(all_y, all_probs, labels=[0,1])\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds, labels=[0,1]).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn)\n",
    "\n",
    "    results_file.write(\"AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\\n\".format(aroc, spec, sens))\n",
    "    \n",
    "    break\n",
    "    \n",
    "results_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def deepface_reps(GENERAL_DIR, syn_name):\n",
    "\n",
    "    model = create_deepface()\n",
    "    syn_rep, ID_rep = [], []\n",
    "\n",
    "    syn_dir = GENERAL_DIR + \"\\\\{}\\{}-patients\".format(syn_name, syn_name)\n",
    "    ID_dir = GENERAL_DIR + \"\\\\{}\\{}-selected-ID-controls\".format(syn_name, syn_name)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_syn = [f for f in listdir(syn_dir) if (isfile(join(syn_dir, f)))and syn_name in f]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f))) and \".jpg\" in f]\n",
    "    \n",
    "    print(\"Syn_list: {}, ID_list: {}\".format(len(files_syn), len(files_ID)))\n",
    "\n",
    "        \n",
    "    # for each kdv image save deepface rep as list:\n",
    "    for filename in files_syn:\n",
    "        im = Image.open(join(syn_dir, filename))\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        output = model.predict(np.expand_dims(im, axis=0))\n",
    "        syn_rep.append([filename] + output[0].tolist())  \n",
    "\n",
    "\n",
    "    # for each ID image save deepface rep as list:\n",
    "    for filename in files_ID:\n",
    "        im = Image.open(join(ID_dir, filename))\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        output = model.predict(np.expand_dims(im, axis=0))\n",
    "        ID_rep.append([filename] + output[0].tolist())\n",
    "\n",
    "    print(\"Syn_reps: {}, ID_reps: {}\".format(len(syn_rep), len(ID_rep)))\n",
    "\n",
    "    # location to save representation\n",
    "    csv_file_syn = GENERAL_DIR + \"\\\\{}\\\\representations\\\\{}-patients-deepface.csv\".format(syn_name, syn_name)\n",
    "    csv_file_ID = GENERAL_DIR + \"\\\\{}\\\\representations\\\\ID-controls-deepface.csv\".format(syn_name)\n",
    "    \n",
    "    # save representation of kdv patients\n",
    "    with open(csv_file_syn, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(syn_rep)\n",
    "\n",
    "    # save representation of ID controls\n",
    "    with open(csv_file_ID, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(ID_rep)\n",
    "\n",
    "    print(\"Done with saving all deepface representations for {}.\".format(syn_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
