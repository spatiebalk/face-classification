{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load weights of DeepFace\n",
    "2. Add classification layer (2)\n",
    "3. For each syn in syn_list calculate the stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from os import path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image \n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_v2_behavior()\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import keras.initializers\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (152, 152) # set by the model \n",
    "CHANNELS = 3 # RGB image\n",
    "NUM_CLASSES = 8631 # classification layer will be removed \n",
    "LEARN_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "DOWNLOAD_PATH = 'https://github.com/swghosh/DeepFace/releases/download/weights-vggface2-2d-aligned/VGGFace2_DeepFace_weights_val-0.9034.h5.zip'\n",
    "MD5_HASH = '0b21fb70cd6901c96c19ac14c9ea8b89'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_init = keras.initializers.RandomNormal(mean=0, stddev=0.01)\n",
    "bias_init = keras.initializers.Constant(value=0.5)\n",
    "\n",
    "def conv2d_layer(**args):\n",
    "    return keras.layers.Conv2D(**args, \n",
    "        kernel_initializer=wt_init, \n",
    "        bias_initializer=bias_init,\n",
    "        activation=keras.activations.relu)\n",
    "def lc2d_layer(**args):\n",
    "    return keras.layers.LocallyConnected2D(**args, \n",
    "        kernel_initializer=wt_init, \n",
    "        bias_initializer=bias_init,\n",
    "        activation=keras.activations.relu)\n",
    "def dense_layer(**args):\n",
    "    return keras.layers.Dense(**args, \n",
    "        kernel_initializer=wt_init, \n",
    "        bias_initializer=bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifying_deepface(image_size=IMAGE_SIZE, channels=CHANNELS, num_classes=NUM_CLASSES, learn_rate=LEARN_RATE, momentum=MOMENTUM):\n",
    "    deepface = keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(*image_size, channels), name='I0'),\n",
    "        conv2d_layer(filters=32, kernel_size=11, name='C1'),\n",
    "        keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same',  name='M2'),\n",
    "        conv2d_layer(filters=16, kernel_size=9, name='C3'),\n",
    "        lc2d_layer(filters=16, kernel_size=9, name='L4'),\n",
    "        lc2d_layer(filters=16, kernel_size=7, strides=2, name='L5'),\n",
    "        lc2d_layer(filters=16, kernel_size=5, name='L6'),\n",
    "        keras.layers.Flatten(name='F0'),\n",
    "        dense_layer(units=4096, activation=keras.activations.relu, name='F7'),\n",
    "        keras.layers.Dropout(rate=0.5, name='D0'),\n",
    "        dense_layer(units=num_classes, activation=keras.activations.softmax, name='F8')\n",
    "    ], name='DeepFace')\n",
    "    # deepface.summary()\n",
    "\n",
    "    sgd_opt = keras.optimizers.SGD(lr=learn_rate, momentum=momentum)\n",
    "    cce_loss = keras.losses.categorical_crossentropy\n",
    "\n",
    "    #deepface.compile(optimizer=sgd_opt, loss=cce_loss, metrics=['accuracy'])\n",
    "    \n",
    "    return deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights():\n",
    "    filename = 'deepface.zip'\n",
    "    downloaded_file_path = keras.utils.get_file(filename, DOWNLOAD_PATH, \n",
    "        md5_hash=MD5_HASH, extract=True)\n",
    "    downloaded_h5_file = path.join(path.dirname(downloaded_file_path), \n",
    "        path.basename(DOWNLOAD_PATH).rstrip('.zip'))\n",
    "    return downloaded_h5_file\n",
    "\n",
    "\n",
    "def create_deepface():\n",
    "    model = create_classifying_deepface()\n",
    "    weights = get_weights()\n",
    "    model.load_weights(weights)\n",
    "    \n",
    "    num_classes = 2\n",
    "    x = model.layers[-2].output\n",
    "    x = dense_layer(units=num_classes, activation=keras.activations.softmax, name='preds')(x)\n",
    "    #x = Dense(2, activation='softmax', name='predictions')(x)\n",
    "    model2 = Model(model.input, x)\n",
    "    \n",
    "    sgd_opt = keras.optimizers.SGD(lr=LEARN_RATE, momentum=MOMENTUM)\n",
    "    cce_loss = keras.losses.categorical_crossentropy\n",
    "    #model2.compile(optimizer=sgd_opt, loss=cce_loss, metrics=['accuracy'])\n",
    " \n",
    "\n",
    "    model2.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "        metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    #model2.summary()\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(syn, GENERAL_DIR):\n",
    "    \n",
    "    data, labels = [], [] \n",
    "    \n",
    "    syn_dir = GENERAL_DIR + \"\\\\{}\\{}-patients\".format(syn, syn)\n",
    "    ID_dir = GENERAL_DIR + \"\\\\{}\\{}-selected-ID-controls\".format(syn, syn)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_syn = [f for f in listdir(syn_dir) if (isfile(join(syn_dir, f)))and \".jpg\" in f]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f))) and \".jpg\" in f]\n",
    "    \n",
    "    print(\"Syn_list: {}, ID_list: {}\".format(len(files_syn), len(files_ID)))\n",
    "\n",
    "    for filename in files_syn:\n",
    "        im = Image.open(join(syn_dir, filename))\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        data.append(np.array(im))\n",
    "        labels.append(1)\n",
    "\n",
    "    for filename in files_ID:\n",
    "        im = Image.open(join(ID_dir, filename))\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        data.append(np.array(im))\n",
    "        labels.append(0)    \n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(\n",
    "    img_array, model, last_conv_layer_name, classifier_layer_names\n",
    "):\n",
    "    \n",
    "    img_array = tf.convert_to_tensor(img_array, dtype=np.float32)\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "    # Second, we create a model that maps the activations of the last conv\n",
    "    # layer to the final class predictions\n",
    "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = keras.Model(classifier_input, x)\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute activations of the last conv layer and make the tape watch it\n",
    "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        # Compute class predictions\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "    # This is the gradient of the top predicted class with regard to\n",
    "    # the output feature map of the last conv layer\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "I0 (InputLayer)              (None, 152, 152, 3)       0         \n",
      "_________________________________________________________________\n",
      "C1 (Conv2D)                  (None, 142, 142, 32)      11648     \n",
      "_________________________________________________________________\n",
      "M2 (MaxPooling2D)            (None, 71, 71, 32)        0         \n",
      "_________________________________________________________________\n",
      "C3 (Conv2D)                  (None, 63, 63, 16)        41488     \n",
      "_________________________________________________________________\n",
      "L4 (LocallyConnected2D)      (None, 55, 55, 16)        62774800  \n",
      "_________________________________________________________________\n",
      "L5 (LocallyConnected2D)      (None, 25, 25, 16)        7850000   \n",
      "_________________________________________________________________\n",
      "L6 (LocallyConnected2D)      (None, 21, 21, 16)        2829456   \n",
      "_________________________________________________________________\n",
      "F0 (Flatten)                 (None, 7056)              0         \n",
      "_________________________________________________________________\n",
      "F7 (Dense)                   (None, 4096)              28905472  \n",
      "_________________________________________________________________\n",
      "D0 (Dropout)                 (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 102,421,058\n",
      "Trainable params: 102,421,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 152, 152, 3)\n",
      "predictions: [[1.0000000e+00 2.2654115e-08]]\n"
     ]
    }
   ],
   "source": [
    "model  = create_deepface()\n",
    "model.summary()\n",
    "\n",
    "img = Image.open(r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\test\\YY1_1.jpg\")\n",
    "img = np.array(img.resize(IMAGE_SIZE))\n",
    "img = np.expand_dims(img, axis=0)\n",
    "print(img.shape)\n",
    "\n",
    "preds = model.predict(img)\n",
    "print(\"predictions: {}\".format(preds))\n",
    "\n",
    "last_conv_layer_name = \"C3\"\n",
    "classifier_layer_names = [\"L4\", \"L5\", \"L6\", \"F0\", \"F7\", \"D0\", \"preds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7ff17cbeefd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mheatmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_gradcam_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_conv_layer_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier_layer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-3b61c4411dd2>\u001b[0m in \u001b[0;36mmake_gradcam_heatmap\u001b[1;34m(img_array, model, last_conv_layer_name, classifier_layer_names)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# This is the gradient of the top predicted class with regard to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# the output feature map of the last conv layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_class_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_conv_layer_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# This is a vector where each entry is the mean intensity of the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_IfGrad\u001b[1;34m(op, *grads)\u001b[0m\n\u001b[0;32m    169\u001b[0m   \u001b[1;31m# Resolve references to forward graph tensors in grad graphs and ensure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m   \u001b[1;31m# they are in-scope, i.e., belong to one of outer graphs of the grad graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m   \u001b[0mtrue_grad_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_resolve_grad_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_grad_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m   \u001b[0mfalse_grad_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_resolve_grad_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfalse_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfalse_grad_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_resolve_grad_inputs\u001b[1;34m(cond_graph, grad_graph)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;31m# `cond_graph`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgrad_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m       \u001b[1;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcond_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m       \u001b[1;31m# `internal_captures` are not treated as intermediates and hence not added\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m       \u001b[1;31m# to If op outputs. So we get the outer tensor corresponding to those\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name, classifier_layer_names)\n",
    "\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__) ## 2.2.0\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "model  = create_deepface()\n",
    "\n",
    "im = Image.open(r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\\test\\YY1_1.jpg\")\n",
    "im = np.array(im.resize(IMAGE_SIZE))\n",
    "\n",
    "#im = preprocessing.image.img_to_array(im)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n",
    "im = tf.convert_to_tensor(im, dtype=np.float32)\n",
    "\n",
    "conv_layer = model.get_layer(\"L6\")\n",
    "heatmap_model = Model([model.input], [conv_layer.output, model.output])\n",
    "\n",
    "with tf.GradientTape() as gtape:\n",
    "    conv_output, predictions = heatmap_model(im)\n",
    "    i = tf.math.argmax(predictions[0], -1) #  .eval(session=tf.InteractiveSession()) #.numpy()\n",
    "    \n",
    "    loss = predictions[:, i] #np.argmax(predictions[0])]\n",
    "    print(loss)\n",
    "    print(type(loss))\n",
    "    gtape.watch(conv_output)\n",
    "    gtape.watch(loss)\n",
    "grads = gtape.gradient(loss, conv_output)\n",
    "pooled_grads = K.mean(grads, axis=(0,1,2))\n",
    "\n",
    "heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "max_heat = np.max(heatmap)\n",
    "if max_heat == 0:\n",
    "    max_heat = 1e-10\n",
    "heatmap /= max_heat\n",
    "\n",
    "print(heatmap.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 32\n",
    "GENERAL_DIR = r\"H:\\Genetica Projecten\\Facial Recognition\\Studenten en Onderzoekers\\Fien\" \n",
    "\n",
    "# load img data\n",
    "syn_list = ['ADNP', 'ANKRD11', 'CDK13', 'DEAF1', 'DYRK1A', 'EHMT1', 'FBXO11', 'SON', 'WAC', 'YY1', 'KDVS']\n",
    "results_file = open(\"results/deepface_classification.txt\", \"w\")\n",
    "\n",
    "for syn in ['YY1']:\n",
    "    data, labels = load_data(syn, GENERAL_DIR)\n",
    "\n",
    "    results_file.write(\"Syndrome {} with {} patients and {} controls\\n\".format(syn, labels.tolist().count(1), labels.tolist().count(0)))\n",
    "    all_y, all_probs, all_preds = [], [], [] \n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    for train_index, test_index in tqdm(loo.split(data)):\n",
    "        X_train, X_test = np.array(data[train_index]), data[test_index]\n",
    "        y_train, y_test = np.array(labels[train_index]), labels[test_index]\n",
    "        \n",
    "        %reset_selective \"model\"\n",
    "        \n",
    "        model = create_deepface()\n",
    "        model.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE, epochs=1, shuffle=True)\n",
    "\n",
    "        y_pred_array = model.predict(X_test)\n",
    "        y_pred = tf.math.argmax(y_pred_array, -1).numpy()\n",
    "\n",
    "        all_y.append(y_test[0])\n",
    "        all_probs.append(y_pred_array[0][1])\n",
    "        all_preds.append(y_pred)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    aroc = roc_auc_score(all_y, all_probs, labels=[0,1])\n",
    "    tn, fp, fn, tp = confusion_matrix(all_y, all_preds, labels=[0,1]).ravel()\n",
    "    spec = tn / (tn+fp)  \n",
    "    sens = tp / (tp+fn)\n",
    "\n",
    "    results_file.write(\"AROC: {:.4f}, spec: {:.4f}, sens: {:.4f}\\n\\n\".format(aroc, spec, sens))\n",
    "    \n",
    "    break\n",
    "    \n",
    "results_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def deepface_reps(GENERAL_DIR, syn_name):\n",
    "\n",
    "    model = create_deepface()\n",
    "    syn_rep, ID_rep = [], []\n",
    "\n",
    "    syn_dir = GENERAL_DIR + \"\\\\{}\\{}-patients\".format(syn_name, syn_name)\n",
    "    ID_dir = GENERAL_DIR + \"\\\\{}\\{}-selected-ID-controls\".format(syn_name, syn_name)\n",
    "\n",
    "    # get list of filenames\n",
    "    files_syn = [f for f in listdir(syn_dir) if (isfile(join(syn_dir, f)))and syn_name in f]\n",
    "    files_ID = [f for f in listdir(ID_dir) if (isfile(join(ID_dir, f))) and \".jpg\" in f]\n",
    "    \n",
    "    print(\"Syn_list: {}, ID_list: {}\".format(len(files_syn), len(files_ID)))\n",
    "\n",
    "        \n",
    "    # for each kdv image save deepface rep as list:\n",
    "    for filename in files_syn:\n",
    "        im = Image.open(join(syn_dir, filename))\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        output = model.predict(np.expand_dims(im, axis=0))\n",
    "        syn_rep.append([filename] + output[0].tolist())  \n",
    "\n",
    "\n",
    "    # for each ID image save deepface rep as list:\n",
    "    for filename in files_ID:\n",
    "        im = Image.open(join(ID_dir, filename))\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        output = model.predict(np.expand_dims(im, axis=0))\n",
    "        ID_rep.append([filename] + output[0].tolist())\n",
    "\n",
    "    print(\"Syn_reps: {}, ID_reps: {}\".format(len(syn_rep), len(ID_rep)))\n",
    "\n",
    "    # location to save representation\n",
    "    csv_file_syn = GENERAL_DIR + \"\\\\{}\\\\representations\\\\{}-patients-deepface.csv\".format(syn_name, syn_name)\n",
    "    csv_file_ID = GENERAL_DIR + \"\\\\{}\\\\representations\\\\ID-controls-deepface.csv\".format(syn_name)\n",
    "    \n",
    "    # save representation of kdv patients\n",
    "    with open(csv_file_syn, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(syn_rep)\n",
    "\n",
    "    # save representation of ID controls\n",
    "    with open(csv_file_ID, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(ID_rep)\n",
    "\n",
    "    print(\"Done with saving all deepface representations for {}.\".format(syn_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
